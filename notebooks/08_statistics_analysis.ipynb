{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08_statistics_analysis\n",
    "Stats, hypothesis testing, regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Content to be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File: notebooks/08_statistics_analysis.ipynb\n",
    "\n",
    "# OctaveMasterPro: Statistics & Analysis\n",
    "\n",
    "Master statistical analysis and hypothesis testing! This notebook covers descriptive statistics, probability distributions, hypothesis testing, regression analysis, and advanced statistical methods essential for data-driven decision making.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Compute comprehensive descriptive statistics\n",
    "- Work with probability distributions and random sampling\n",
    "- Perform hypothesis testing and significance analysis\n",
    "- Implement regression analysis and model validation\n",
    "- Apply advanced statistical methods for real-world problems\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Descriptive Statistics Fundamentals\n",
    "\n",
    "```octave\n",
    "% Descriptive statistics fundamentals\n",
    "fprintf('=== Descriptive Statistics Fundamentals ===\\n');\n",
    "\n",
    "% Generate sample datasets with different characteristics\n",
    "rng(42);  % Set seed for reproducibility\n",
    "dataset1 = randn(1000, 1) * 2 + 10;           % Normal: Œº=10, œÉ=2\n",
    "dataset2 = exprnd(3, 1000, 1);                % Exponential: Œª=1/3\n",
    "dataset3 = [randn(500, 1) + 5; randn(500, 1) + 15];  % Bimodal\n",
    "dataset4 = unifrnd(0, 20, 1000, 1);           % Uniform [0,20]\n",
    "\n",
    "datasets = {dataset1, dataset2, dataset3, dataset4};\n",
    "names = {'Normal', 'Exponential', 'Bimodal', 'Uniform'};\n",
    "\n",
    "fprintf('Analyzing four different distributions:\\n');\n",
    "\n",
    "for i = 1:length(datasets)\n",
    "    data = datasets{i};\n",
    "    fprintf('\\n%s Distribution:\\n', names{i});\n",
    "    \n",
    "    % Central tendency measures\n",
    "    data_mean = mean(data);\n",
    "    data_median = median(data);\n",
    "    data_mode = mode(data);  % Most frequent value (for discrete data)\n",
    "    \n",
    "    fprintf('  Mean: %.4f\\n', data_mean);\n",
    "    fprintf('  Median: %.4f\\n', data_median);\n",
    "    \n",
    "    % Variability measures\n",
    "    data_std = std(data);\n",
    "    data_var = var(data);\n",
    "    data_range = range(data);\n",
    "    data_iqr = iqr(data);\n",
    "    \n",
    "    fprintf('  Standard Deviation: %.4f\\n', data_std);\n",
    "    fprintf('  Variance: %.4f\\n', data_var);\n",
    "    fprintf('  Range: %.4f\\n', data_range);\n",
    "    fprintf('  IQR: %.4f\\n', data_iqr);\n",
    "    \n",
    "    % Shape measures\n",
    "    data_skewness = skewness(data);\n",
    "    data_kurtosis = kurtosis(data);\n",
    "    \n",
    "    fprintf('  Skewness: %.4f', data_skewness);\n",
    "    if data_skewness > 0.5\n",
    "        fprintf(' (right-skewed)');\n",
    "    elseif data_skewness < -0.5\n",
    "        fprintf(' (left-skewed)');\n",
    "    else\n",
    "        fprintf(' (approximately symmetric)');\n",
    "    end\n",
    "    fprintf('\\n');\n",
    "    \n",
    "    fprintf('  Kurtosis: %.4f', data_kurtosis);\n",
    "    if data_kurtosis > 3\n",
    "        fprintf(' (heavy-tailed)');\n",
    "    elseif data_kurtosis < 3\n",
    "        fprintf(' (light-tailed)');\n",
    "    else\n",
    "        fprintf(' (normal-like)');\n",
    "    end\n",
    "    fprintf('\\n');\n",
    "    \n",
    "    % Quantiles\n",
    "    quartiles = quantile(data, [0.25, 0.5, 0.75]);\n",
    "    percentiles = quantile(data, [0.05, 0.1, 0.9, 0.95]);\n",
    "    \n",
    "    fprintf('  Quartiles (Q1, Q2, Q3): [%.3f, %.3f, %.3f]\\n', quartiles);\n",
    "    fprintf('  5th, 10th, 90th, 95th percentiles: [%.3f, %.3f, %.3f, %.3f]\\n', percentiles);\n",
    "    \n",
    "    % Outlier detection using IQR method\n",
    "    q1 = quartiles(1);\n",
    "    q3 = quartiles(3);\n",
    "    iqr_val = q3 - q1;\n",
    "    lower_bound = q1 - 1.5 * iqr_val;\n",
    "    upper_bound = q3 + 1.5 * iqr_val;\n",
    "    \n",
    "    outliers = data(data < lower_bound | data > upper_bound);\n",
    "    fprintf('  Outliers (IQR method): %d (%.1f%%)\\n', length(outliers), 100*length(outliers)/length(data));\n",
    "end\n",
    "\n",
    "% Comprehensive statistics function\n",
    "function stats = comprehensive_stats(data)\n",
    "    % Compute comprehensive statistics for a dataset\n",
    "    % Input: data - numeric vector\n",
    "    % Output: stats - structure with all statistics\n",
    "    \n",
    "    stats = struct();\n",
    "    \n",
    "    % Basic measures\n",
    "    stats.n = length(data);\n",
    "    stats.mean = mean(data);\n",
    "    stats.median = median(data);\n",
    "    stats.std = std(data);\n",
    "    stats.var = var(data);\n",
    "    stats.min = min(data);\n",
    "    stats.max = max(data);\n",
    "    stats.range = range(data);\n",
    "    \n",
    "    % Robust measures\n",
    "    stats.mad = mad(data);  % Median absolute deviation\n",
    "    stats.iqr = iqr(data);\n",
    "    stats.trimmed_mean = trimmean(data, 20);  % 20% trimmed mean\n",
    "    \n",
    "    % Shape measures\n",
    "    stats.skewness = skewness(data);\n",
    "    stats.kurtosis = kurtosis(data);\n",
    "    \n",
    "    % Percentiles\n",
    "    stats.quartiles = quantile(data, [0.25, 0.5, 0.75]);\n",
    "    stats.percentiles_5_95 = quantile(data, [0.05, 0.95]);\n",
    "    \n",
    "    % Confidence intervals\n",
    "    alpha = 0.05;  % 95% confidence level\n",
    "    sem = stats.std / sqrt(stats.n);  % Standard error of mean\n",
    "    t_crit = tinv(1 - alpha/2, stats.n - 1);  % t-critical value\n",
    "    stats.ci_mean = [stats.mean - t_crit*sem, stats.mean + t_crit*sem];\n",
    "    \n",
    "    % Normality indicators\n",
    "    stats.cv = stats.std / abs(stats.mean);  % Coefficient of variation\n",
    "    stats.zscore_range = [min((data - stats.mean)/stats.std), max((data - stats.mean)/stats.mean))];\n",
    "end\n",
    "\n",
    "% Test comprehensive statistics function\n",
    "fprintf('\\nComprehensive statistics example:\\n');\n",
    "test_data = dataset1;\n",
    "stats_comp = comprehensive_stats(test_data);\n",
    "\n",
    "fprintf('Sample size: %d\\n', stats_comp.n);\n",
    "fprintf('Mean ¬± SEM: %.4f ¬± %.4f\\n', stats_comp.mean, stats_comp.std/sqrt(stats_comp.n));\n",
    "fprintf('95%% CI for mean: [%.4f, %.4f]\\n', stats_comp.ci_mean);\n",
    "fprintf('Coefficient of variation: %.4f\\n', stats_comp.cv);\n",
    "fprintf('Trimmed mean (20%%): %.4f\\n', stats_comp.trimmed_mean);\n",
    "```\n",
    "\n",
    "## 2. Probability Distributions\n",
    "\n",
    "```octave\n",
    "% Probability distributions and random sampling\n",
    "fprintf('\\n=== Probability Distributions ===\\n');\n",
    "\n",
    "% Normal Distribution\n",
    "fprintf('1. Normal Distribution Analysis:\\n');\n",
    "mu_norm = 5; sigma_norm = 2;\n",
    "x_norm = linspace(mu_norm - 4*sigma_norm, mu_norm + 4*sigma_norm, 100);\n",
    "pdf_norm = normpdf(x_norm, mu_norm, sigma_norm);\n",
    "cdf_norm = normcdf(x_norm, mu_norm, sigma_norm);\n",
    "\n",
    "fprintf('   Parameters: Œº=%.1f, œÉ=%.1f\\n', mu_norm, sigma_norm);\n",
    "fprintf('   P(X < Œº): %.4f\\n', normcdf(mu_norm, mu_norm, sigma_norm));\n",
    "fprintf('   P(Œº-œÉ < X < Œº+œÉ): %.4f (68%% rule)\\n', ...\n",
    "        normcdf(mu_norm + sigma_norm, mu_norm, sigma_norm) - ...\n",
    "        normcdf(mu_norm - sigma_norm, mu_norm, sigma_norm));\n",
    "\n",
    "% Generate random samples and verify\n",
    "samples_norm = normrnd(mu_norm, sigma_norm, 10000, 1);\n",
    "fprintf('   Sample mean: %.4f (expected: %.1f)\\n', mean(samples_norm), mu_norm);\n",
    "fprintf('   Sample std: %.4f (expected: %.1f)\\n', std(samples_norm), sigma_norm);\n",
    "\n",
    "% Student's t-Distribution\n",
    "fprintf('\\n2. Student''s t-Distribution:\\n');\n",
    "dof_values = [1, 5, 10, 30];\n",
    "x_t = linspace(-4, 4, 100);\n",
    "\n",
    "for dof = dof_values\n",
    "    pdf_t = tpdf(x_t, dof);\n",
    "    variance_t = dof / (dof - 2);  % For dof > 2\n",
    "    \n",
    "    fprintf('   DoF=%d: Variance=%.4f, P(-2<X<2)=%.4f\\n', ...\n",
    "            dof, variance_t, tcdf(2, dof) - tcdf(-2, dof));\n",
    "end\n",
    "\n",
    "% Chi-square Distribution\n",
    "fprintf('\\n3. Chi-square Distribution:\\n');\n",
    "dof_chi = [1, 5, 10, 20];\n",
    "x_chi = linspace(0, 30, 100);\n",
    "\n",
    "for dof = dof_chi\n",
    "    mean_chi = dof;\n",
    "    var_chi = 2 * dof;\n",
    "    critical_95 = chi2inv(0.95, dof);\n",
    "    \n",
    "    fprintf('   DoF=%d: Mean=%.0f, Var=%.0f, 95th percentile=%.2f\\n', ...\n",
    "            dof, mean_chi, var_chi, critical_95);\n",
    "end\n",
    "\n",
    "% F-Distribution\n",
    "fprintf('\\n4. F-Distribution:\\n');\n",
    "df1_vals = [5, 10, 20];\n",
    "df2_vals = [10, 20, 30];\n",
    "\n",
    "for i = 1:length(df1_vals)\n",
    "    df1 = df1_vals(i);\n",
    "    df2 = df2_vals(i);\n",
    "    \n",
    "    mean_f = df2 / (df2 - 2);  % For df2 > 2\n",
    "    critical_f = finv(0.95, df1, df2);\n",
    "    \n",
    "    fprintf('   F(%d,%d): Mean=%.4f, 95th percentile=%.4f\\n', ...\n",
    "            df1, df2, mean_f, critical_f);\n",
    "end\n",
    "\n",
    "% Distribution fitting and testing\n",
    "fprintf('\\n5. Distribution Fitting:\\n');\n",
    "\n",
    "% Generate mixed data that might not be normal\n",
    "mixed_data = [normrnd(0, 1, 800, 1); normrnd(3, 0.5, 200, 1)];\n",
    "\n",
    "% Test for normality using various methods\n",
    "function normality_results = test_normality(data)\n",
    "    % Test data for normality using multiple methods\n",
    "    % Input: data - numeric vector\n",
    "    % Output: normality_results - test results structure\n",
    "    \n",
    "    results = struct();\n",
    "    \n",
    "    % Shapiro-Wilk test (simplified implementation)\n",
    "    n = length(data);\n",
    "    data_sorted = sort(data);\n",
    "    \n",
    "    % Jarque-Bera test\n",
    "    data_mean = mean(data);\n",
    "    data_std = std(data);\n",
    "    \n",
    "    % Skewness and kurtosis\n",
    "    s = skewness(data);\n",
    "    k = kurtosis(data);\n",
    "    \n",
    "    % Jarque-Bera statistic\n",
    "    jb_stat = n * (s^2/6 + (k-3)^2/24);\n",
    "    jb_critical = chi2inv(0.95, 2);  % Chi-square critical value with 2 DoF\n",
    "    \n",
    "    results.jb_statistic = jb_stat;\n",
    "    results.jb_critical = jb_critical;\n",
    "    results.jb_normal = jb_stat < jb_critical;\n",
    "    \n",
    "    % Anderson-Darling test (simplified)\n",
    "    data_standardized = (data_sorted - data_mean) / data_std;\n",
    "    cdf_vals = normcdf(data_standardized);\n",
    "    \n",
    "    % Kolmogorov-Smirnov test\n",
    "    theoretical_cdf = normcdf(data_sorted, data_mean, data_std);\n",
    "    empirical_cdf = (1:n)' / n;\n",
    "    ks_stat = max(abs(empirical_cdf - theoretical_cdf));\n",
    "    ks_critical = 1.36 / sqrt(n);  # Approximate critical value at Œ±=0.05\n",
    "    \n",
    "    results.ks_statistic = ks_stat;\n",
    "    results.ks_critical = ks_critical;\n",
    "    results.ks_normal = ks_stat < ks_critical;\n",
    "    \n",
    "    results.skewness = s;\n",
    "    results.kurtosis = k;\n",
    "end\n",
    "\n",
    "norm_results = test_normality(mixed_data);\n",
    "fprintf('   Normality test results:\\n');\n",
    "fprintf('     Jarque-Bera: %.4f (critical: %.4f) - %s\\n', ...\n",
    "        norm_results.jb_statistic, norm_results.jb_critical, ...\n",
    "        iif(norm_results.jb_normal, 'Normal', 'Not Normal'));\n",
    "fprintf('     Kolmogorov-Smirnov: %.4f (critical: %.4f) - %s\\n', ...\n",
    "        norm_results.ks_statistic, norm_results.ks_critical, ...\n",
    "        iif(norm_results.ks_normal, 'Normal', 'Not Normal'));\n",
    "\n",
    "% Bootstrap sampling\n",
    "fprintf('\\n6. Bootstrap Sampling:\\n');\n",
    "original_sample = normrnd(10, 2, 50, 1);\n",
    "n_bootstrap = 1000;\n",
    "\n",
    "bootstrap_means = zeros(n_bootstrap, 1);\n",
    "bootstrap_stds = zeros(n_bootstrap, 1);\n",
    "\n",
    "for i = 1:n_bootstrap\n",
    "    % Resample with replacement\n",
    "    bootstrap_sample = original_sample(randi(length(original_sample), length(original_sample), 1));\n",
    "    bootstrap_means(i) = mean(bootstrap_sample);\n",
    "    bootstrap_stds(i) = std(bootstrap_sample);\n",
    "end\n",
    "\n",
    "fprintf('   Original sample: mean=%.4f, std=%.4f\\n', mean(original_sample), std(original_sample));\n",
    "fprintf('   Bootstrap means: mean=%.4f, std=%.4f\\n', mean(bootstrap_means), std(bootstrap_means));\n",
    "fprintf('   Bootstrap 95%% CI for mean: [%.4f, %.4f]\\n', ...\n",
    "        quantile(bootstrap_means, [0.025, 0.975]));\n",
    "\n",
    "% Theoretical SEM\n",
    "theoretical_sem = std(original_sample) / sqrt(length(original_sample));\n",
    "fprintf('   Theoretical SEM: %.4f\\n', theoretical_sem);\n",
    "fprintf('   Bootstrap SEM: %.4f\\n', std(bootstrap_means));\n",
    "```\n",
    "\n",
    "## 3. Hypothesis Testing\n",
    "\n",
    "```octave\n",
    "% Hypothesis testing procedures\n",
    "fprintf('\\n=== Hypothesis Testing ===\\n');\n",
    "\n",
    "% One-sample t-test\n",
    "fprintf('1. One-Sample t-Test:\\n');\n",
    "sample_data = [2.3, 2.7, 2.1, 2.8, 2.5, 2.4, 2.9, 2.2, 2.6, 2.5];\n",
    "mu_0 = 2.0;  % Null hypothesis mean\n",
    "alpha = 0.05;\n",
    "\n",
    "function [t_stat, p_value, reject_null, ci] = one_sample_ttest(data, mu0, alpha)\n",
    "    % Perform one-sample t-test\n",
    "    % Input: data - sample data, mu0 - null hypothesis mean, alpha - significance level\n",
    "    % Output: test statistics and results\n",
    "    \n",
    "    n = length(data);\n",
    "    sample_mean = mean(data);\n",
    "    sample_std = std(data);\n",
    "    \n",
    "    % t-statistic\n",
    "    t_stat = (sample_mean - mu0) / (sample_std / sqrt(n));\n",
    "    \n",
    "    % Degrees of freedom\n",
    "    df = n - 1;\n",
    "    \n",
    "    % Two-tailed p-value\n",
    "    p_value = 2 * (1 - tcdf(abs(t_stat), df));\n",
    "    \n",
    "    % Critical value\n",
    "    t_critical = tinv(1 - alpha/2, df);\n",
    "    \n",
    "    % Decision\n",
    "    reject_null = abs(t_stat) > t_critical;\n",
    "    \n",
    "    # Confidence interval\n",
    "    margin_error = t_critical * sample_std / sqrt(n);\n",
    "    ci = [sample_mean - margin_error, sample_mean + margin_error];\n",
    "end\n",
    "\n",
    "[t_stat, p_val, reject, ci_mean] = one_sample_ttest(sample_data, mu_0, alpha);\n",
    "\n",
    "fprintf('   H0: Œº = %.1f, H1: Œº ‚â† %.1f\\n', mu_0, mu_0);\n",
    "fprintf('   Sample mean: %.4f\\n', mean(sample_data));\n",
    "fprintf('   t-statistic: %.4f\\n', t_stat);\n",
    "fprintf('   p-value: %.6f\\n', p_val);\n",
    "fprintf('   Decision: %s H0 (Œ± = %.2f)\\n', iif(reject, 'Reject', 'Fail to reject'), alpha);\n",
    "fprintf('   95%% CI for mean: [%.4f, %.4f]\\n', ci_mean);\n",
    "\n",
    "% Two-sample t-test\n",
    "fprintf('\\n2. Two-Sample t-Test:\\n');\n",
    "group1 = normrnd(10, 2, 25, 1);\n",
    "group2 = normrnd(12, 2.5, 30, 1);\n",
    "\n",
    "function [t_stat, p_value, reject_null] = two_sample_ttest(data1, data2, alpha, equal_var)\n",
    "    % Perform two-sample t-test\n",
    "    % Input: data1, data2 - sample data, alpha - significance level, equal_var - assume equal variances\n",
    "    % Output: test statistics and results\n",
    "    \n",
    "    n1 = length(data1); n2 = length(data2);\n",
    "    mean1 = mean(data1); mean2 = mean(data2);\n",
    "    var1 = var(data1); var2 = var(data2);\n",
    "    \n",
    "    if equal_var\n",
    "        # Pooled variance\n",
    "        pooled_var = ((n1-1)*var1 + (n2-1)*var2) / (n1 + n2 - 2);\n",
    "        se_diff = sqrt(pooled_var * (1/n1 + 1/n2));\n",
    "        df = n1 + n2 - 2;\n",
    "    else\n",
    "        # Welch's t-test (unequal variances)\n",
    "        se_diff = sqrt(var1/n1 + var2/n2);\n",
    "        df = (var1/n1 + var2/n2)^2 / ((var1/n1)^2/(n1-1) + (var2/n2)^2/(n2-1));\n",
    "    end\n",
    "    \n",
    "    t_stat = (mean1 - mean2) / se_diff;\n",
    "    p_value = 2 * (1 - tcdf(abs(t_stat), df));\n",
    "    \n",
    "    t_critical = tinv(1 - alpha/2, df);\n",
    "    reject_null = abs(t_stat) > t_critical;\n",
    "end\n",
    "\n",
    "[t_stat_2, p_val_2, reject_2] = two_sample_ttest(group1, group2, alpha, false);\n",
    "\n",
    "fprintf('   Group 1: n=%d, mean=%.4f, std=%.4f\\n', length(group1), mean(group1), std(group1));\n",
    "fprintf('   Group 2: n=%d, mean=%.4f, std=%.4f\\n', length(group2), mean(group2), std(group2));\n",
    "fprintf('   H0: Œº1 = Œº2, H1: Œº1 ‚â† Œº2\\n');\n",
    "fprintf('   t-statistic: %.4f\\n', t_stat_2);\n",
    "fprintf('   p-value: %.6f\\n', p_val_2);\n",
    "fprintf('   Decision: %s H0\\n', iif(reject_2, 'Reject', 'Fail to reject'));\n",
    "\n",
    "% Chi-square goodness of fit test\n",
    "fprintf('\\n3. Chi-Square Goodness of Fit Test:\\n');\n",
    "observed = [20, 18, 16, 19, 15, 22];  % Observed frequencies\n",
    "expected = [18, 18, 18, 18, 18, 18];  # Expected frequencies (equal)\n",
    "\n",
    "function [chi2_stat, p_value, reject_null] = chi2_goodness_of_fit(observed, expected, alpha)\n",
    "    % Perform chi-square goodness of fit test\n",
    "    % Input: observed - observed frequencies, expected - expected frequencies, alpha - significance level\n",
    "    % Output: test statistics and results\n",
    "    \n",
    "    % Chi-square statistic\n",
    "    chi2_stat = sum((observed - expected).^2 ./ expected);\n",
    "    \n",
    "    # Degrees of freedom\n",
    "    df = length(observed) - 1;\n",
    "    \n",
    "    # p-value\n",
    "    p_value = 1 - chi2cdf(chi2_stat, df);\n",
    "    \n",
    "    # Critical value\n",
    "    chi2_critical = chi2inv(1 - alpha, df);\n",
    "    \n",
    "    reject_null = chi2_stat > chi2_critical;\n",
    "end\n",
    "\n",
    "[chi2_stat, p_val_chi2, reject_chi2] = chi2_goodness_of_fit(observed, expected, alpha);\n",
    "\n",
    "fprintf('   Observed frequencies: ['); fprintf('%d ', observed); fprintf(']\\n');\n",
    "fprintf('   Expected frequencies: ['); fprintf('%d ', expected); fprintf(']\\n');\n",
    "fprintf('   H0: Data follows expected distribution\\n');\n",
    "fprintf('   œá¬≤ statistic: %.4f\\n', chi2_stat);\n",
    "fprintf('   p-value: %.6f\\n', p_val_chi2);\n",
    "fprintf('   Decision: %s H0\\n', iif(reject_chi2, 'Reject', 'Fail to reject'));\n",
    "\n",
    "% ANOVA (Analysis of Variance)\n",
    "fprintf('\\n4. One-Way ANOVA:\\n');\n",
    "group_a = normrnd(5, 1, 20, 1);\n",
    "group_b = normrnd(6, 1, 20, 1);  \n",
    "group_c = normrnd(7, 1, 20, 1);\n",
    "\n",
    "function [f_stat, p_value, reject_null] = one_way_anova(groups, alpha)\n",
    "    % Perform one-way ANOVA\n",
    "    % Input: groups - cell array of group data, alpha - significance level\n",
    "    # Output: test statistics and results\n",
    "    \n",
    "    k = length(groups);  % Number of groups\n",
    "    n_total = sum(cellfun(@length, groups));\n",
    "    \n",
    "    % Grand mean\n",
    "    all_data = [];\n",
    "    group_means = zeros(k, 1);\n",
    "    group_sizes = zeros(k, 1);\n",
    "    \n",
    "    for i = 1:k\n",
    "        all_data = [all_data; groups{i}];\n",
    "        group_means(i) = mean(groups{i});\n",
    "        group_sizes(i) = length(groups{i});\n",
    "    end\n",
    "    \n",
    "    grand_mean = mean(all_data);\n",
    "    \n",
    "    % Sum of squares\n",
    "    ss_between = sum(group_sizes .* (group_means - grand_mean).^2);\n",
    "    \n",
    "    ss_within = 0;\n",
    "    for i = 1:k\n",
    "        ss_within = ss_within + sum((groups{i} - group_means(i)).^2);\n",
    "    end\n",
    "    \n",
    "    ss_total = sum((all_data - grand_mean).^2);\n",
    "    \n",
    "    % Degrees of freedom\n",
    "    df_between = k - 1;\n",
    "    df_within = n_total - k;\n",
    "    \n",
    "    # Mean squares\n",
    "    ms_between = ss_between / df_between;\n",
    "    ms_within = ss_within / df_within;\n",
    "    \n",
    "    # F-statistic\n",
    "    f_stat = ms_between / ms_within;\n",
    "    \n",
    "    # p-value\n",
    "    p_value = 1 - fcdf(f_stat, df_between, df_within);\n",
    "    \n",
    "    # Critical value\n",
    "    f_critical = finv(1 - alpha, df_between, df_within);\n",
    "    \n",
    "    reject_null = f_stat > f_critical;\n",
    "end\n",
    "\n",
    "groups_anova = {group_a, group_b, group_c};\n",
    "[f_stat_anova, p_val_anova, reject_anova] = one_way_anova(groups_anova, alpha);\n",
    "\n",
    "fprintf('   Group A: mean=%.4f, std=%.4f\\n', mean(group_a), std(group_a));\n",
    "fprintf('   Group B: mean=%.4f, std=%.4f\\n', mean(group_b), std(group_b));\n",
    "fprintf('   Group C: mean=%.4f, std=%.4f\\n', mean(group_c), std(group_c));\n",
    "fprintf('   H0: All group means are equal\\n');\n",
    "fprintf('   F-statistic: %.4f\\n', f_stat_anova);\n",
    "fprintf('   p-value: %.6f\\n', p_val_anova);\n",
    "fprintf('   Decision: %s H0\\n', iif(reject_anova, 'Reject', 'Fail to reject'));\n",
    "\n",
    "% Power analysis\n",
    "fprintf('\\n5. Statistical Power Analysis:\\n');\n",
    "\n",
    "function power = compute_power(effect_size, sample_size, alpha)\n",
    "    % Compute statistical power for one-sample t-test\n",
    "    % Input: effect_size - Cohen's d, sample_size - n, alpha - significance level\n",
    "    % Output: power - statistical power (1 - Œ≤)\n",
    "    \n",
    "    # Non-centrality parameter\n",
    "    delta = effect_size * sqrt(sample_size);\n",
    "    \n",
    "    # Critical t-value\n",
    "    df = sample_size - 1;\n",
    "    t_critical = tinv(1 - alpha/2, df);\n",
    "    \n",
    "    # Power calculation (simplified)\n",
    "    power = 1 - tcdf(t_critical, df, delta) + tcdf(-t_critical, df, delta);\n",
    "end\n",
    "\n",
    "effect_sizes = [0.2, 0.5, 0.8];  % Small, medium, large effects\n",
    "sample_sizes = [10, 20, 30, 50];\n",
    "\n",
    "fprintf('   Statistical Power Analysis:\\n');\n",
    "fprintf('   Effect Size | n=10 | n=20 | n=30 | n=50\\n');\n",
    "fprintf('   ------------|------|------|------|------\\n');\n",
    "\n",
    "for es = effect_sizes\n",
    "    fprintf('   %.1f (%-6s) |', es, iif(es==0.2, 'small', iif(es==0.5, 'medium', 'large')));\n",
    "    for n = sample_sizes\n",
    "        power = compute_power(es, n, alpha);\n",
    "        fprintf(' %.3f|', power);\n",
    "    end\n",
    "    fprintf('\\n');\n",
    "end\n",
    "```\n",
    "\n",
    "## 4. Regression Analysis\n",
    "\n",
    "```octave\n",
    "% Regression analysis techniques\n",
    "fprintf('\\n=== Regression Analysis ===\\n');\n",
    "\n",
    "% Simple Linear Regression\n",
    "fprintf('1. Simple Linear Regression:\\n');\n",
    "\n",
    "% Generate sample data with known relationship\n",
    "x_reg = linspace(1, 10, 50)';\n",
    "y_true = 2 + 3*x_reg;  % True relationship: y = 2 + 3x\n",
    "y_reg = y_true + normrnd(0, 1, size(x_reg));  # Add noise\n",
    "\n",
    "function [coeffs, stats] = simple_linear_regression(x, y)\n",
    "    % Perform simple linear regression\n",
    "    % Input: x - predictor variable, y - response variable\n",
    "    % Output: coeffs - [intercept, slope], stats - regression statistics\n",
    "    \n",
    "    n = length(x);\n",
    "    X = [ones(n, 1), x];  % Design matrix\n",
    "    \n",
    "    % Least squares solution\n",
    "    coeffs = (X' * X) \\ (X' * y);\n",
    "    \n",
    "    % Fitted values and residuals\n",
    "    y_hat = X * coeffs;\n",
    "    residuals = y - y_hat;\n",
    "    \n",
    "    # Statistics\n",
    "    stats = struct();\n",
    "    stats.y_hat = y_hat;\n",
    "    stats.residuals = residuals;\n",
    "    \n",
    "    # Sum of squares\n",
    "    ss_total = sum((y - mean(y)).^2);\n",
    "    ss_residual = sum(residuals.^2);\n",
    "    ss_regression = sum((y_hat - mean(y)).^2);\n",
    "    \n",
    "    # R-squared\n",
    "    stats.r_squared = ss_regression / ss_total;\n",
    "    stats.r_squared_adj = 1 - (ss_residual / (n - 2)) / (ss_total / (n - 1));\n",
    "    \n",
    "    # Standard errors\n",
    "    mse = ss_residual / (n - 2);\n",
    "    cov_matrix = mse * inv(X' * X);\n",
    "    stats.se_coeffs = sqrt(diag(cov_matrix));\n",
    "    \n",
    "    # t-statistics\n",
    "    stats.t_stats = coeffs ./ stats.se_coeffs;\n",
    "    \n",
    "    # p-values (two-tailed)\n",
    "    stats.p_values = 2 * (1 - tcdf(abs(stats.t_stats), n - 2));\n",
    "    \n",
    "    # Residual standard error\n",
    "    stats.rse = sqrt(mse);\n",
    "    \n",
    "    # F-statistic\n",
    "    stats.f_stat = (ss_regression / 1) / (ss_residual / (n - 2));\n",
    "    stats.f_p_value = 1 - fcdf(stats.f_stat, 1, n - 2);\n",
    "end\n",
    "\n",
    "[coeffs_simple, stats_simple] = simple_linear_regression(x_reg, y_reg);\n",
    "\n",
    "fprintf('   True relationship: y = 2 + 3x\\n');\n",
    "fprintf('   Fitted relationship: y = %.4f + %.4f*x\\n', coeffs_simple);\n",
    "fprintf('   R¬≤ = %.4f, Adjusted R¬≤ = %.4f\\n', stats_simple.r_squared, stats_simple.r_squared_adj);\n",
    "fprintf('   Residual standard error: %.4f\\n', stats_simple.rse);\n",
    "fprintf('   F-statistic: %.4f (p = %.6f)\\n', stats_simple.f_stat, stats_simple.f_p_value);\n",
    "\n",
    "fprintf('   Coefficient Statistics:\\n');\n",
    "fprintf('     Intercept: %.4f (SE=%.4f, t=%.4f, p=%.6f)\\n', ...\n",
    "        coeffs_simple(1), stats_simple.se_coeffs(1), stats_simple.t_stats(1), stats_simple.p_values(1));\n",
    "fprintf('     Slope: %.4f (SE=%.4f, t=%.4f, p=%.6f)\\n', ...\n",
    "        coeffs_simple(2), stats_simple.se_coeffs(2), stats_simple.t_stats(2), stats_simple.p_values(2));\n",
    "\n",
    "% Multiple Linear Regression\n",
    "fprintf('\\n2. Multiple Linear Regression:\\n');\n",
    "\n",
    "% Generate multiple predictors\n",
    "n_obs = 100;\n",
    "x1_multi = randn(n_obs, 1);\n",
    "x2_multi = randn(n_obs, 1);\n",
    "x3_multi = 0.5 * x1_multi + randn(n_obs, 1);  % Correlated with x1\n",
    "\n",
    "# True relationship: y = 1 + 2*x1 - 1.5*x2 + 0.8*x3\n",
    "y_multi = 1 + 2*x1_multi - 1.5*x2_multi + 0.8*x3_multi + normrnd(0, 0.5, n_obs, 1);\n",
    "\n",
    "function [coeffs, stats] = multiple_regression(X, y)\n",
    "    % Perform multiple linear regression\n",
    "    % Input: X - design matrix (with intercept column), y - response variable\n",
    "    % Output: coeffs - regression coefficients, stats - regression statistics\n",
    "    \n",
    "    n = size(X, 1);\n",
    "    p = size(X, 2);\n",
    "    \n",
    "    # Least squares solution\n",
    "    coeffs = (X' * X) \\ (X' * y);\n",
    "    \n",
    "    # Fitted values and residuals\n",
    "    y_hat = X * coeffs;\n",
    "    residuals = y - y_hat;\n",
    "    \n",
    "    % Statistics\n",
    "    stats = struct();\n",
    "    stats.y_hat = y_hat;\n",
    "    stats.residuals = residuals;\n",
    "    \n",
    "    # Sum of squares\n",
    "    ss_total = sum((y - mean(y)).^2);\n",
    "    ss_residual = sum(residuals.^2);\n",
    "    ss_regression = sum((y_hat - mean(y)).^2);\n",
    "    \n",
    "    # R-squared\n",
    "    stats.r_squared = ss_regression / ss_total;\n",
    "    stats.r_squared_adj = 1 - (ss_residual / (n - p)) / (ss_total / (n - 1));\n",
    "    \n",
    "    # Standard errors\n",
    "    mse = ss_residual / (n - p);\n",
    "    cov_matrix = mse * inv(X' * X);\n",
    "    stats.se_coeffs = sqrt(diag(cov_matrix));\n",
    "    \n",
    "    # t-statistics and p-values\n",
    "    stats.t_stats = coeffs ./ stats.se_coeffs;\n",
    "    stats.p_values = 2 * (1 - tcdf(abs(stats.t_stats), n - p));\n",
    "    \n",
    "    # Overall F-test\n",
    "    stats.f_stat = (ss_regression / (p - 1)) / (ss_residual / (n - p));\n",
    "    stats.f_p_value = 1 - fcdf(stats.f_stat, p - 1, n - p);\n",
    "    \n",
    "    # Additional diagnostics\n",
    "    stats.rse = sqrt(mse);\n",
    "    stats.leverage = diag(X * inv(X' * X) * X');\n",
    "    stats.cooks_distance = (residuals.^2 / (p * mse)) .* (stats.leverage ./ (1 - stats.leverage).^2);\n",
    "end\n",
    "\n",
    "X_multi = [ones(n_obs, 1), x1_multi, x2_multi, x3_multi];\n",
    "[coeffs_multi, stats_multi] = multiple_regression(X_multi, y_multi);\n",
    "\n",
    "fprintf('   True coefficients: [1.0, 2.0, -1.5, 0.8]\\n');\n",
    "fprintf('   Fitted coefficients: ['); fprintf('%.4f ', coeffs_multi'); fprintf(']\\n');\n",
    "fprintf('   R¬≤ = %.4f, Adjusted R¬≤ = %.4f\\n', stats_multi.r_squared, stats_multi.r_squared_adj);\n",
    "fprintf('   Overall F-test: F = %.4f (p = %.6f)\\n', stats_multi.f_stat, stats_multi.f_p_value);\n",
    "\n",
    "var_names = {'Intercept', 'x1', 'x2', 'x3'};\n",
    "fprintf('   Individual coefficient tests:\\n');\n",
    "for i = 1:length(coeffs_multi)\n",
    "    fprintf('     %s: %.4f (SE=%.4f, t=%.4f, p=%.6f)\\n', var_names{i}, ...\n",
    "            coeffs_multi(i), stats_multi.se_coeffs(i), stats_multi.t_stats(i), stats_multi.p_values(i));\n",
    "end\n",
    "\n",
    "% Polynomial Regression\n",
    "fprintf('\\n3. Polynomial Regression:\\n');\n",
    "x_poly = linspace(-3, 3, 50)';\n",
    "y_poly_true = 0.5*x_poly.^3 - 2*x_poly.^2 + x_poly + 1;\n",
    "y_poly = y_poly_true + normrnd(0, 0.5, size(x_poly));\n",
    "\n",
    "% Fit polynomials of different degrees\n",
    "degrees = [1, 2, 3, 4];\n",
    "aic_values = zeros(size(degrees));\n",
    "bic_values = zeros(size(degrees));\n",
    "\n",
    "for i = 1:length(degrees)\n",
    "    degree = degrees(i);\n",
    "    \n",
    "    # Create polynomial design matrix\n",
    "    X_poly = zeros(length(x_poly), degree + 1);\n",
    "    for j = 0:degree\n",
    "        X_poly(:, j + 1) = x_poly.^j;\n",
    "    end\n",
    "    \n",
    "    # Fit model\n",
    "    [coeffs_poly, stats_poly] = multiple_regression(X_poly, y_poly);\n",
    "    \n",
    "    # Information criteria\n",
    "    n = length(y_poly);\n",
    "    k = degree + 1;  # Number of parameters\n",
    "    log_likelihood = -n/2 * (log(2*pi) + log(stats_poly.rse^2) + 1);\n",
    "    \n",
    "    aic_values(i) = 2*k - 2*log_likelihood;\n",
    "    bic_values(i) = k*log(n) - 2*log_likelihood;\n",
    "    \n",
    "    fprintf('   Degree %d: R¬≤ = %.4f, AIC = %.2f, BIC = %.2f\\n', ...\n",
    "            degree, stats_poly.r_squared, aic_values(i), bic_values(i));\n",
    "end\n",
    "\n",
    "[~, best_aic] = min(aic_values);\n",
    "[~, best_bic] = min(bic_values);\n",
    "\n",
    "fprintf('   Best model by AIC: Degree %d\\n', degrees(best_aic));\n",
    "fprintf('   Best model by BIC: Degree %d\\n', degrees(best_bic));\n",
    "\n",
    "% Regression diagnostics\n",
    "fprintf('\\n4. Regression Diagnostics:\\n');\n",
    "\n",
    "function diagnostics = regression_diagnostics(X, y, stats)\n",
    "    % Perform regression diagnostics\n",
    "    % Input: X - design matrix, y - response, stats - regression statistics\n",
    "    % Output: diagnostics - diagnostic tests and plots data\n",
    "    \n",
    "    diagnostics = struct();\n",
    "    residuals = stats.residuals;\n",
    "    y_hat = stats.y_hat;\n",
    "    n = length(y);\n",
    "    p = size(X, 2);\n",
    "    \n",
    "    # Standardized residuals\n",
    "    rse = stats.rse;\n",
    "    diagnostics.std_residuals = residuals / rse;\n",
    "    \n",
    "    # Studentized residuals\n",
    "    leverage = stats.leverage;\n",
    "    diagnostics.student_residuals = residuals ./ (rse * sqrt(1 - leverage));\n",
    "    \n",
    "    # Normality test of residuals (Jarque-Bera)\n",
    "    skew_res = skewness(residuals);\n",
    "    kurt_res = kurtosis(residuals);\n",
    "    jb_stat = n * (skew_res^2/6 + (kurt_res - 3)^2/24);\n",
    "    diagnostics.normality_p = 1 - chi2cdf(jb_stat, 2);\n",
    "    \n",
    "    # Breusch-Pagan test for heteroscedasticity (simplified)\n",
    "    residuals_squared = residuals.^2;\n",
    "    X_hetero = [ones(n, 1), y_hat];  # Regress squared residuals on fitted values\n",
    "    coeffs_hetero = (X_hetero' * X_hetero) \\ (X_hetero' * residuals_squared);\n",
    "    y_hat_hetero = X_hetero * coeffs_hetero;\n",
    "    \n",
    "    ss_reg_hetero = sum((y_hat_hetero - mean(residuals_squared)).^2);\n",
    "    ss_tot_hetero = sum((residuals_squared - mean(residuals_squared)).^2);\n",
    "    r2_hetero = ss_reg_hetero / ss_tot_hetero;\n",
    "    \n",
    "    bp_stat = n * r2_hetero;\n",
    "    diagnostics.heterosced_p = 1 - chi2cdf(bp_stat, 1);\n",
    "    \n",
    "    # Outlier detection\n",
    "    diagnostics.outliers_std = find(abs(diagnostics.std_residuals) > 2);\n",
    "    diagnostics.outliers_student = find(abs(diagnostics.student_residuals) > 2);\n",
    "    diagnostics.high_leverage = find(leverage > 2*p/n);\n",
    "    diagnostics.high_cooks = find(stats.cooks_distance > 4/n);\n",
    "end\n",
    "\n",
    "diag_results = regression_diagnostics(X_multi, y_multi, stats_multi);\n",
    "\n",
    "fprintf('   Residual normality test (p-value): %.4f', diag_results.normality_p);\n",
    "if diag_results.normality_p > 0.05\n",
    "    fprintf(' (Normal)');\n",
    "else\n",
    "    fprintf(' (Non-normal)');\n",
    "end\n",
    "fprintf('\\n');\n",
    "\n",
    "fprintf('   Heteroscedasticity test (p-value): %.4f', diag_results.heterosced_p);\n",
    "if diag_results.heterosced_p > 0.05\n",
    "    fprintf(' (Homoscedastic)');\n",
    "else\n",
    "    fprintf(' (Heteroscedastic)');\n",
    "end\n",
    "fprintf('\\n');\n",
    "\n",
    "fprintf('   Outliers (|std residual| > 2): %d observations\\n', length(diag_results.outliers_std));\n",
    "fprintf('   High leverage points: %d observations\\n', length(diag_results.high_leverage));\n",
    "fprintf('   High Cook''s distance: %d observations\\n', length(diag_results.high_cooks));\n",
    "```\n",
    "\n",
    "## 5. Advanced Statistical Methods\n",
    "\n",
    "```octave\n",
    "% Advanced statistical methods\n",
    "fprintf('\\n=== Advanced Statistical Methods ===\\n');\n",
    "\n",
    "% Time Series Analysis Basics\n",
    "fprintf('1. Basic Time Series Analysis:\\n');\n",
    "\n",
    "% Generate time series data\n",
    "t_ts = 1:100;\n",
    "trend = 0.02 * t_ts;\n",
    "seasonal = 2 * sin(2*pi*t_ts/12) + 1 * cos(2*pi*t_ts/6);\n",
    "noise = 0.5 * randn(size(t_ts));\n",
    "ts_data = trend + seasonal + noise;\n",
    "\n",
    "% Decompose time series components\n",
    "function [trend, seasonal, residual] = decompose_timeseries(data, period)\n",
    "    % Simple time series decomposition\n",
    "    # Input: data - time series data, period - seasonal period\n",
    "    % Output: trend, seasonal, residual components\n",
    "    \n",
    "    n = length(data);\n",
    "    \n",
    "    # Simple moving average for trend\n",
    "    window = round(period);\n",
    "    trend = zeros(size(data));\n",
    "    \n",
    "    for i = 1:n\n",
    "        start_idx = max(1, i - floor(window/2));\n",
    "        end_idx = min(n, i + floor(window/2));\n",
    "        trend(i) = mean(data(start_idx:end_idx));\n",
    "    end\n",
    "    \n",
    "    # Remove trend\n",
    "    detrended = data - trend;\n",
    "    \n",
    "    # Estimate seasonal component\n",
    "    seasonal = zeros(size(data));\n",
    "    for i = 1:period\n",
    "        season_indices = i:period:n;\n",
    "        seasonal(season_indices) = mean(detrended(season_indices));\n",
    "    end\n",
    "    \n",
    "    # Residual\n",
    "    residual = data - trend - seasonal;\n",
    "end\n",
    "\n",
    "[ts_trend, ts_seasonal, ts_residual] = decompose_timeseries(ts_data, 12);\n",
    "\n",
    "fprintf('   Original data variance: %.4f\\n', var(ts_data));\n",
    "fprintf('   Trend variance: %.4f\\n', var(ts_trend));\n",
    "fprintf('   Seasonal variance: %.4f\\n', var(ts_seasonal));\n",
    "fprintf('   Residual variance: %.4f\\n', var(ts_residual));\n",
    "fprintf('   Variance explained: %.1f%%\\n', 100*(1 - var(ts_residual)/var(ts_data)));\n",
    "\n",
    "% Autocorrelation analysis\n",
    "function acf = autocorrelation(data, max_lag)\n",
    "    % Compute autocorrelation function\n",
    "    % Input: data - time series data, max_lag - maximum lag\n",
    "    % Output: acf - autocorrelation values\n",
    "    \n",
    "    n = length(data);\n",
    "    data_centered = data - mean(data);\n",
    "    \n",
    "    acf = zeros(max_lag + 1, 1);\n",
    "    \n",
    "    for lag = 0:max_lag\n",
    "        if lag == 0\n",
    "            acf(lag + 1) = 1;\n",
    "        else\n",
    "            numerator = sum(data_centered(1:n-lag) .* data_centered(1+lag:n));\n",
    "            denominator = sum(data_centered.^2);\n",
    "            acf(lag + 1) = numerator / denominator;\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "acf_values = autocorrelation(ts_residual, 20);\n",
    "fprintf('   Autocorrelations (lags 1-5): ['); fprintf('%.3f ', acf_values(2:6)'); fprintf(']\\n');\n",
    "\n",
    "% Significant autocorrelations (rough check)\n",
    "n_ts = length(ts_residual);\n",
    "significance_bound = 1.96 / sqrt(n_ts);\n",
    "significant_lags = find(abs(acf_values(2:end)) > significance_bound);\n",
    "\n",
    "if ~isempty(significant_lags)\n",
    "    fprintf('   Significant autocorrelations at lags: ['); fprintf('%d ', significant_lags'); fprintf(']\\n');\n",
    "else\n",
    "    fprintf('   No significant autocorrelations detected\\n');\n",
    "end\n",
    "\n",
    "% Principal Component Analysis\n",
    "fprintf('\\n2. Principal Component Analysis:\\n');\n",
    "\n",
    "% Generate multivariate data\n",
    "n_pca = 200;\n",
    "X_pca = randn(n_pca, 5);\n",
    "# Create correlations\n",
    "X_pca(:, 2) = 0.8 * X_pca(:, 1) + 0.6 * X_pca(:, 2);\n",
    "X_pca(:, 3) = 0.5 * X_pca(:, 1) + 0.7 * X_pca(:, 3);\n",
    "\n",
    "function [scores, loadings, eigenvals, explained_var] = pca_analysis(X)\n",
    "    % Perform Principal Component Analysis\n",
    "    % Input: X - data matrix (observations x variables)\n",
    "    # Output: PCA results\n",
    "    \n",
    "    # Center the data\n",
    "    X_centered = X - mean(X, 1);\n",
    "    \n",
    "    # Covariance matrix\n",
    "    C = cov(X_centered);\n",
    "    \n",
    "    # Eigendecomposition\n",
    "    [loadings, D] = eig(C);\n",
    "    eigenvals = diag(D);\n",
    "    \n",
    "    # Sort by eigenvalues (descending)\n",
    "    [eigenvals, idx] = sort(eigenvals, 'descend');\n",
    "    loadings = loadings(:, idx);\n",
    "    \n",
    "    # Principal component scores\n",
    "    scores = X_centered * loadings;\n",
    "    \n",
    "    # Explained variance\n",
    "    explained_var = eigenvals / sum(eigenvals) * 100;\n",
    "end\n",
    "\n",
    "[pc_scores, pc_loadings, pc_eigenvals, pc_explained] = pca_analysis(X_pca);\n",
    "\n",
    "fprintf('   Number of variables: %d\\n', size(X_pca, 2));\n",
    "fprintf('   Principal Component Analysis Results:\\n');\n",
    "fprintf('     PC1: %.1f%% variance explained\\n', pc_explained(1));\n",
    "fprintf('     PC2: %.1f%% variance explained\\n', pc_explained(2));\n",
    "fprintf('     PC3: %.1f%% variance explained\\n', pc_explained(3));\n",
    "fprintf('     Cumulative (PC1-PC3): %.1f%%\\n', sum(pc_explained(1:3)));\n",
    "\n",
    "% Determine number of components to retain\n",
    "cutoff_variance = 80;  % 80% of variance\n",
    "cumsum_explained = cumsum(pc_explained);\n",
    "n_components = find(cumsum_explained >= cutoff_variance, 1);\n",
    "fprintf('   Components needed for %.0f%% variance: %d\\n', cutoff_variance, n_components);\n",
    "\n",
    "% Kaiser criterion (eigenvalues > 1)\n",
    "kaiser_components = sum(pc_eigenvals > 1);\n",
    "fprintf('   Kaiser criterion (Œª > 1): %d components\\n', kaiser_components);\n",
    "\n",
    "% Cluster Analysis (K-means)\n",
    "fprintf('\\n3. K-means Clustering:\\n');\n",
    "\n",
    "% Generate clustered data\n",
    "cluster_centers = [0, 0; 3, 3; -2, 4];\n",
    "n_per_cluster = 50;\n",
    "cluster_data = [];\n",
    "true_labels = [];\n",
    "\n",
    "for i = 1:size(cluster_centers, 1)\n",
    "    center = cluster_centers(i, :);\n",
    "    cluster_points = mvnrnd(center, 0.5*eye(2), n_per_cluster);\n",
    "    cluster_data = [cluster_data; cluster_points];\n",
    "    true_labels = [true_labels; i*ones(n_per_cluster, 1)];\n",
    "end\n",
    "\n",
    "function [labels, centroids, within_ss] = kmeans_clustering(data, k, max_iter)\n",
    "    % Perform k-means clustering\n",
    "    % Input: data - data matrix, k - number of clusters, max_iter - max iterations\n",
    "    % Output: cluster assignments and statistics\n",
    "    \n",
    "    n = size(data, 1);\n",
    "    d = size(data, 2);\n",
    "    \n",
    "    # Initialize centroids randomly\n",
    "    centroids = data(randperm(n, k), :);\n",
    "    labels = zeros(n, 1);\n",
    "    \n",
    "    for iter = 1:max_iter\n",
    "        old_labels = labels;\n",
    "        \n",
    "        # Assign points to nearest centroid\n",
    "        distances = zeros(n, k);\n",
    "        for j = 1:k\n",
    "            diff = data - repmat(centroids(j, :), n, 1);\n",
    "            distances(:, j) = sum(diff.^2, 2);\n",
    "        end\n",
    "        \n",
    "        [~, labels] = min(distances, [], 2);\n",
    "        \n",
    "        # Update centroids\n",
    "        for j = 1:k\n",
    "            cluster_points = data(labels == j, :);\n",
    "            if ~isempty(cluster_points)\n",
    "                centroids(j, :) = mean(cluster_points, 1);\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # Check convergence\n",
    "        if isequal(labels, old_labels)\n",
    "            break;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Calculate within-cluster sum of squares\n",
    "    within_ss = 0;\n",
    "    for j = 1:k\n",
    "        cluster_points = data(labels == j, :);\n",
    "        if ~isempty(cluster_points)\n",
    "            centroid = centroids(j, :);\n",
    "            within_ss = within_ss + sum(sum((cluster_points - repmat(centroid, size(cluster_points, 1), 1)).^2));\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "k_true = 3;\n",
    "[cluster_labels, cluster_centroids, wss] = kmeans_clustering(cluster_data, k_true, 100);\n",
    "\n",
    "fprintf('   True number of clusters: %d\\n', k_true);\n",
    "fprintf('   Detected centroids:\\n');\n",
    "for i = 1:k_true\n",
    "    fprintf('     Cluster %d: [%.3f, %.3f]\\n', i, cluster_centroids(i, :));\n",
    "end\n",
    "fprintf('   Within-cluster sum of squares: %.2f\\n', wss);\n",
    "\n",
    "% Evaluate clustering quality\n",
    "function ari = adjusted_rand_index(true_labels, pred_labels)\n",
    "    % Calculate Adjusted Rand Index\n",
    "    % Input: true_labels, pred_labels - cluster assignments\n",
    "    % Output: ari - adjusted rand index (-1 to 1, 1 = perfect match)\n",
    "    \n",
    "    % Contingency table\n",
    "    n = length(true_labels);\n",
    "    k1 = max(true_labels);\n",
    "    k2 = max(pred_labels);\n",
    "    \n",
    "    cont_table = zeros(k1, k2);\n",
    "    for i = 1:n\n",
    "        cont_table(true_labels(i), pred_labels(i)) = cont_table(true_labels(i), pred_labels(i)) + 1;\n",
    "    end\n",
    "    \n",
    "    # Calculate ARI components\n",
    "    a = sum(sum(cont_table.^2));\n",
    "    b = sum(sum(cont_table, 1).^2);\n",
    "    c = sum(sum(cont_table, 2).^2);\n",
    "    d = sum(sum(cont_table))^2;\n",
    "    \n",
    "    # ARI formula (simplified)\n",
    "    numerator = a - (b*c)/d;\n",
    "    denominator = (b + c)/2 - (b*c)/d;\n",
    "    \n",
    "    ari = numerator / denominator;\n",
    "end\n",
    "\n",
    "ari_score = adjusted_rand_index(true_labels, cluster_labels);\n",
    "fprintf('   Adjusted Rand Index: %.4f (1 = perfect, 0 = random)\\n', ari_score);\n",
    "\n",
    "% Determine optimal number of clusters using elbow method\n",
    "fprintf('   Elbow method for optimal k:\\n');\n",
    "k_values = 1:6;\n",
    "wss_values = zeros(size(k_values));\n",
    "\n",
    "for i = 1:length(k_values)\n",
    "    k = k_values(i);\n",
    "    [~, ~, wss_k] = kmeans_clustering(cluster_data, k, 50);\n",
    "    wss_values(i) = wss_k;\n",
    "end\n",
    "\n",
    "fprintf('     k=1: WSS=%.2f\\n', wss_values(1));\n",
    "for i = 2:length(k_values)\n",
    "    reduction = (wss_values(i-1) - wss_values(i)) / wss_values(i-1) * 100;\n",
    "    fprintf('     k=%d: WSS=%.2f (%.1f%% reduction)\\n', k_values(i), wss_values(i), reduction);\n",
    "end\n",
    "\n",
    "# Find elbow (largest reduction in WSS)\n",
    "wss_reductions = diff(wss_values) ./ wss_values(1:end-1);\n",
    "[~, elbow_idx] = max(-wss_reductions);  # Negative because we want largest reduction\n",
    "optimal_k = k_values(elbow_idx + 1);\n",
    "\n",
    "fprintf('   Suggested optimal k: %d\\n', optimal_k);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "**Statistics & Analysis Mastery Completed:**\n",
    "\n",
    "This comprehensive notebook covered all essential statistical analysis techniques:\n",
    "\n",
    "- ‚úÖ **Descriptive Statistics**: Central tendency, variability, distribution shape, quantiles\n",
    "- ‚úÖ **Probability Distributions**: Normal, t, chi-square, F-distributions, fitting and testing\n",
    "- ‚úÖ **Hypothesis Testing**: t-tests, ANOVA, chi-square tests, power analysis\n",
    "- ‚úÖ **Regression Analysis**: Linear, multiple, polynomial regression with diagnostics\n",
    "- ‚úÖ **Advanced Methods**: Time series, PCA, clustering, bootstrap sampling\n",
    "\n",
    "**Key Statistical Insights:**\n",
    "1. **Distribution Assessment**: Use multiple normality tests and visual inspection\n",
    "2. **Hypothesis Testing**: Consider effect size, power, and practical significance\n",
    "3. **Regression**: Validate assumptions through residual analysis and diagnostics\n",
    "4. **Model Selection**: Use information criteria (AIC/BIC) and cross-validation\n",
    "5. **Multivariate Analysis**: PCA for dimensionality reduction, clustering for patterns\n",
    "\n",
    "**Professional Statistical Skills:**\n",
    "- Design and interpret hypothesis tests with appropriate power\n",
    "- Build and validate regression models with diagnostic checking\n",
    "- Apply multivariate techniques for complex data structures\n",
    "- Implement bootstrap and resampling methods for robust inference\n",
    "- Recognize and handle violations of statistical assumptions\n",
    "\n",
    "**Real-World Applications:**\n",
    "- **Research**: Experimental design, hypothesis testing, publication-quality analysis\n",
    "- **Business**: A/B testing, market research, quality control, forecasting\n",
    "- **Engineering**: Process optimization, reliability analysis, signal detection\n",
    "- **Healthcare**: Clinical trials, epidemiological studies, diagnostic testing\n",
    "- **Social Sciences**: Survey analysis, behavioral studies, policy evaluation\n",
    "\n",
    "**Best Practices Mastered:**\n",
    "- Always check assumptions before applying statistical tests\n",
    "- Use effect sizes alongside significance tests\n",
    "- Validate models through residual analysis and diagnostics\n",
    "- Consider multiple comparison corrections when appropriate\n",
    "- Report confidence intervals and practical significance\n",
    "\n",
    "**Next Steps:**\n",
    "- Apply these methods to domain-specific datasets\n",
    "- Explore Bayesian statistical approaches\n",
    "- Study advanced topics like survival analysis and mixed models\n",
    "- Proceed to `09_optimization_roots.ipynb` for numerical optimization\n",
    "\n",
    "Your statistical analysis toolkit is now industry-standard! üìàüîç"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
