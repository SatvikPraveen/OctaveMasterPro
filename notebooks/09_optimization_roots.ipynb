{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09_optimization_roots\n",
    "Optimization, root-finding techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Content to be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OctaveMasterPro: Optimization & Root-Finding\n",
    "\n",
    "Master numerical optimization and root-finding techniques! This notebook covers unconstrained and constrained optimization, root-finding algorithms, and advanced numerical methods essential for solving real-world engineering and scientific problems.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Implement root-finding algorithms (bisection, Newton-Raphson, secant)\n",
    "- Master unconstrained optimization techniques\n",
    "- Apply constrained optimization methods\n",
    "- Understand convergence criteria and numerical stability\n",
    "- Solve complex optimization problems in multiple variables\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Root-Finding Fundamentals\n",
    "\n",
    "```octave\n",
    "% Root-finding algorithms and applications\n",
    "fprintf('=== Root-Finding Fundamentals ===\\n');\n",
    "\n",
    "% Define test functions with known roots\n",
    "f1 = @(x) x^3 - 6*x^2 + 11*x - 6;  % Roots: 1, 2, 3\n",
    "f1_prime = @(x) 3*x^2 - 12*x + 11;  % Derivative\n",
    "\n",
    "f2 = @(x) exp(-x) - x;  % Transcendental equation\n",
    "f2_prime = @(x) -exp(-x) - 1;\n",
    "\n",
    "f3 = @(x) cos(x) - x;  % Another transcendental\n",
    "f3_prime = @(x) -sin(x) - 1;\n",
    "\n",
    "fprintf('Test functions defined with theoretical roots\\n');\n",
    "\n",
    "% Bisection Method\n",
    "fprintf('\\n1. Bisection Method:\\n');\n",
    "\n",
    "function [root, iterations, errors] = bisection(func, a, b, tol, max_iter)\n",
    "    % Bisection method for root finding\n",
    "    % Input: func - function handle, [a,b] - bracket, tol - tolerance, max_iter - max iterations\n",
    "    % Output: root - approximate root, iterations - number of iterations, errors - error history\n",
    "    \n",
    "    % Check initial bracket\n",
    "    if func(a) * func(b) > 0\n",
    "        error('Function must have opposite signs at endpoints');\n",
    "    end\n",
    "    \n",
    "    errors = [];\n",
    "    \n",
    "    for iter = 1:max_iter\n",
    "        c = (a + b) / 2;  % Midpoint\n",
    "        fc = func(c);\n",
    "        \n",
    "        % Store error estimate\n",
    "        error_est = (b - a) / 2;\n",
    "        errors(iter) = error_est;\n",
    "        \n",
    "        % Check convergence\n",
    "        if abs(error_est) < tol || abs(fc) < tol\n",
    "            root = c;\n",
    "            iterations = iter;\n",
    "            return;\n",
    "        end\n",
    "        \n",
    "        % Update bracket\n",
    "        if func(a) * fc < 0\n",
    "            b = c;\n",
    "        else\n",
    "            a = c;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    root = c;\n",
    "    iterations = max_iter;\n",
    "end\n",
    "\n",
    "% Test bisection on polynomial\n",
    "[root_bisect, iter_bisect, errors_bisect] = bisection(f1, 0.5, 1.5, 1e-8, 100);\n",
    "fprintf('   Polynomial f(x) = x³ - 6x² + 11x - 6 on [0.5, 1.5]:\\n');\n",
    "fprintf('   Root found: %.8f (iterations: %d)\\n', root_bisect, iter_bisect);\n",
    "fprintf('   Verification: f(%.8f) = %.2e\\n', root_bisect, f1(root_bisect));\n",
    "fprintf('   Theoretical root: 1.0\\n');\n",
    "\n",
    "% Newton-Raphson Method\n",
    "fprintf('\\n2. Newton-Raphson Method:\\n');\n",
    "\n",
    "function [root, iterations, errors] = newton_raphson(func, func_prime, x0, tol, max_iter)\n",
    "    % Newton-Raphson method for root finding\n",
    "    % Input: func - function, func_prime - derivative, x0 - initial guess\n",
    "    % Output: root, iterations, error history\n",
    "    \n",
    "    x = x0;\n",
    "    errors = [];\n",
    "    \n",
    "    for iter = 1:max_iter\n",
    "        fx = func(x);\n",
    "        fpx = func_prime(x);\n",
    "        \n",
    "        % Check for zero derivative\n",
    "        if abs(fpx) < eps\n",
    "            error('Derivative too close to zero');\n",
    "        end\n",
    "        \n",
    "        % Newton update\n",
    "        x_new = x - fx / fpx;\n",
    "        \n",
    "        % Error estimate\n",
    "        error_est = abs(x_new - x);\n",
    "        errors(iter) = error_est;\n",
    "        \n",
    "        % Check convergence\n",
    "        if error_est < tol || abs(fx) < tol\n",
    "            root = x_new;\n",
    "            iterations = iter;\n",
    "            return;\n",
    "        end\n",
    "        \n",
    "        x = x_new;\n",
    "    end\n",
    "    \n",
    "    root = x;\n",
    "    iterations = max_iter;\n",
    "end\n",
    "\n",
    "% Test Newton-Raphson on transcendental equation\n",
    "[root_newton, iter_newton, errors_newton] = newton_raphson(f2, f2_prime, 0.5, 1e-10, 50);\n",
    "fprintf('   Transcendental f(x) = e^(-x) - x, x0 = 0.5:\\n');\n",
    "fprintf('   Root found: %.10f (iterations: %d)\\n', root_newton, iter_newton);\n",
    "fprintf('   Verification: f(%.10f) = %.2e\\n', root_newton, f2(root_newton));\n",
    "\n",
    "% Secant Method\n",
    "fprintf('\\n3. Secant Method:\\n');\n",
    "\n",
    "function [root, iterations, errors] = secant_method(func, x0, x1, tol, max_iter)\n",
    "    % Secant method for root finding\n",
    "    % Input: func - function, x0, x1 - initial points, tol, max_iter\n",
    "    % Output: root, iterations, error history\n",
    "    \n",
    "    errors = [];\n",
    "    \n",
    "    for iter = 1:max_iter\n",
    "        f0 = func(x0);\n",
    "        f1 = func(x1);\n",
    "        \n",
    "        % Check for parallel secant line\n",
    "        if abs(f1 - f0) < eps\n",
    "            error('Function values too close - secant line undefined');\n",
    "        end\n",
    "        \n",
    "        % Secant update\n",
    "        x2 = x1 - f1 * (x1 - x0) / (f1 - f0);\n",
    "        \n",
    "        % Error estimate\n",
    "        error_est = abs(x2 - x1);\n",
    "        errors(iter) = error_est;\n",
    "        \n",
    "        % Check convergence\n",
    "        if error_est < tol || abs(func(x2)) < tol\n",
    "            root = x2;\n",
    "            iterations = iter;\n",
    "            return;\n",
    "        end\n",
    "        \n",
    "        % Update points\n",
    "        x0 = x1;\n",
    "        x1 = x2;\n",
    "    end\n",
    "    \n",
    "    root = x1;\n",
    "    iterations = max_iter;\n",
    "end\n",
    "\n",
    "% Test secant method\n",
    "[root_secant, iter_secant, errors_secant] = secant_method(f3, 0.5, 1.0, 1e-10, 50);\n",
    "fprintf('   Transcendental f(x) = cos(x) - x, x0 = 0.5, x1 = 1.0:\\n');\n",
    "fprintf('   Root found: %.10f (iterations: %d)\\n', root_secant, iter_secant);\n",
    "fprintf('   Verification: f(%.10f) = %.2e\\n', root_secant, f3(root_secant));\n",
    "\n",
    "% Convergence comparison\n",
    "fprintf('\\n4. Convergence Rate Comparison:\\n');\n",
    "fprintf('   Method      | Iterations | Final Error\\n');\n",
    "fprintf('   ------------|------------|------------\\n');\n",
    "fprintf('   Bisection   | %8d   | %.2e\\n', iter_bisect, errors_bisect(end));\n",
    "fprintf('   Newton-Raph | %8d   | %.2e\\n', iter_newton, errors_newton(end));\n",
    "fprintf('   Secant      | %8d   | %.2e\\n', iter_secant, errors_secant(end));\n",
    "\n",
    "% Fixed-point iteration\n",
    "fprintf('\\n5. Fixed-Point Iteration:\\n');\n",
    "\n",
    "function [root, iterations, errors] = fixed_point(g, x0, tol, max_iter)\n",
    "    % Fixed-point iteration: x = g(x)\n",
    "    % Input: g - iteration function, x0 - initial guess\n",
    "    % Output: root, iterations, error history\n",
    "    \n",
    "    x = x0;\n",
    "    errors = [];\n",
    "    \n",
    "    for iter = 1:max_iter\n",
    "        x_new = g(x);\n",
    "        \n",
    "        error_est = abs(x_new - x);\n",
    "        errors(iter) = error_est;\n",
    "        \n",
    "        if error_est < tol\n",
    "            root = x_new;\n",
    "            iterations = iter;\n",
    "            return;\n",
    "        end\n",
    "        \n",
    "        x = x_new;\n",
    "    end\n",
    "    \n",
    "    root = x;\n",
    "    iterations = max_iter;\n",
    "end\n",
    "\n",
    "% Convert f(x) = x³ - 6x² + 11x - 6 = 0 to x = g(x) form\n",
    "% Rearrange to x = (6x² - 11x + 6) / x for x near root at 1\n",
    "g1 = @(x) (6*x^2 - 11*x + 6) / (3*x);  % More stable form\n",
    "[root_fp, iter_fp, errors_fp] = fixed_point(g1, 1.1, 1e-8, 100);\n",
    "\n",
    "fprintf('   Fixed-point form: x = (6x² - 11x + 6)/(3x)\\n');\n",
    "fprintf('   Root found: %.8f (iterations: %d)\\n', root_fp, iter_fp);\n",
    "fprintf('   Verification: f(%.8f) = %.2e\\n', root_fp, f1(root_fp));\n",
    "```\n",
    "\n",
    "## 2. Systems of Nonlinear Equations\n",
    "\n",
    "```octave\n",
    "% Systems of nonlinear equations\n",
    "fprintf('\\n=== Systems of Nonlinear Equations ===\\n');\n",
    "\n",
    "% Define system: f1(x,y) = 0, f2(x,y) = 0\n",
    "% Example: x² + y² - 4 = 0, x² - y - 1 = 0\n",
    "F = @(x) [x(1)^2 + x(2)^2 - 4; x(1)^2 - x(2) - 1];  % System of equations\n",
    "\n",
    "% Jacobian matrix\n",
    "J = @(x) [2*x(1), 2*x(2); 2*x(1), -1];\n",
    "\n",
    "fprintf('System: x² + y² = 4, x² - y = 1\\n');\n",
    "\n",
    "% Newton's method for systems\n",
    "fprintf('\\n1. Newton''s Method for Systems:\\n');\n",
    "\n",
    "function [root, iterations] = newton_system(F, J, x0, tol, max_iter)\n",
    "    % Newton's method for systems of nonlinear equations\n",
    "    % Input: F - system function, J - Jacobian, x0 - initial guess\n",
    "    % Output: root - solution vector, iterations\n",
    "    \n",
    "    x = x0;\n",
    "    \n",
    "    for iter = 1:max_iter\n",
    "        Fx = F(x);\n",
    "        Jx = J(x);\n",
    "        \n",
    "        % Check for convergence\n",
    "        if norm(Fx) < tol\n",
    "            root = x;\n",
    "            iterations = iter;\n",
    "            return;\n",
    "        end\n",
    "        \n",
    "        % Check for singular Jacobian\n",
    "        if rcond(Jx) < eps\n",
    "            error('Jacobian is nearly singular');\n",
    "        end\n",
    "        \n",
    "        % Newton update: x_new = x - J^(-1) * F(x)\n",
    "        delta_x = Jx \\ Fx;\n",
    "        x = x - delta_x;\n",
    "        \n",
    "        % Check step size\n",
    "        if norm(delta_x) < tol\n",
    "            root = x;\n",
    "            iterations = iter;\n",
    "            return;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    root = x;\n",
    "    iterations = max_iter;\n",
    "end\n",
    "\n",
    "% Solve system with different initial guesses\n",
    "initial_guesses = [1.5, 1.0; -1.5, 1.0; 1.0, -1.0];\n",
    "\n",
    "for i = 1:size(initial_guesses, 1)\n",
    "    x0 = initial_guesses(i, :)';\n",
    "    [sol, iters] = newton_system(F, J, x0, 1e-10, 50);\n",
    "    \n",
    "    fprintf('   Initial guess [%.1f, %.1f]: Solution [%.6f, %.6f] (%d iterations)\\n', ...\n",
    "            x0(1), x0(2), sol(1), sol(2), iters);\n",
    "    fprintf('     Verification: ||F(x)|| = %.2e\\n', norm(F(sol)));\n",
    "end\n",
    "\n",
    "% Broyden's method (quasi-Newton)\n",
    "fprintf('\\n2. Broyden''s Quasi-Newton Method:\\n');\n",
    "\n",
    "function [root, iterations] = broyden_method(F, x0, tol, max_iter)\n",
    "    % Broyden's method for systems (approximate Jacobian)\n",
    "    % Input: F - system function, x0 - initial guess\n",
    "    % Output: root, iterations\n",
    "    \n",
    "    x = x0;\n",
    "    n = length(x0);\n",
    "    \n",
    "    % Initial Jacobian approximation (finite differences)\n",
    "    B = zeros(n, n);\n",
    "    h = sqrt(eps);\n",
    "    F0 = F(x);\n",
    "    \n",
    "    for j = 1:n\n",
    "        x_pert = x;\n",
    "        x_pert(j) = x_pert(j) + h;\n",
    "        B(:, j) = (F(x_pert) - F0) / h;\n",
    "    end\n",
    "    \n",
    "    for iter = 1:max_iter\n",
    "        Fx = F(x);\n",
    "        \n",
    "        if norm(Fx) < tol\n",
    "            root = x;\n",
    "            iterations = iter;\n",
    "            return;\n",
    "        end\n",
    "        \n",
    "        % Check for singular approximation\n",
    "        if rcond(B) < eps\n",
    "            error('Jacobian approximation is singular');\n",
    "        end\n",
    "        \n",
    "        % Solve B * delta_x = -F(x)\n",
    "        delta_x = B \\ (-Fx);\n",
    "        x_new = x + delta_x;\n",
    "        \n",
    "        % Update Jacobian approximation (Broyden's update)\n",
    "        Fx_new = F(x_new);\n",
    "        y = Fx_new - Fx;\n",
    "        \n",
    "        if norm(delta_x) > eps\n",
    "            B = B + (y - B * delta_x) * delta_x' / (delta_x' * delta_x);\n",
    "        end\n",
    "        \n",
    "        x = x_new;\n",
    "        \n",
    "        if norm(delta_x) < tol\n",
    "            root = x;\n",
    "            iterations = iter;\n",
    "            return;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    root = x;\n",
    "    iterations = max_iter;\n",
    "end\n",
    "\n",
    "[sol_broyden, iter_broyden] = broyden_method(F, [1.5; 1.0], 1e-8, 100);\n",
    "fprintf('   Broyden solution: [%.6f, %.6f] (%d iterations)\\n', ...\n",
    "        sol_broyden(1), sol_broyden(2), iter_broyden);\n",
    "fprintf('   Verification: ||F(x)|| = %.2e\\n', norm(F(sol_broyden)));\n",
    "```\n",
    "\n",
    "## 3. Unconstrained Optimization\n",
    "\n",
    "```octave\n",
    "% Unconstrained optimization methods\n",
    "fprintf('\\n=== Unconstrained Optimization ===\\n');\n",
    "\n",
    "% Define test functions\n",
    "% Rosenbrock function: f(x,y) = 100(y - x²)² + (1 - x)²\n",
    "rosenbrock = @(x) 100*(x(2) - x(1)^2)^2 + (1 - x(1))^2;\n",
    "rosenbrock_grad = @(x) [-400*x(1)*(x(2) - x(1)^2) - 2*(1 - x(1)); 200*(x(2) - x(1)^2)];\n",
    "\n",
    "% Himmelblau's function: f(x,y) = (x² + y - 11)² + (x + y² - 7)²\n",
    "himmelblau = @(x) (x(1)^2 + x(2) - 11)^2 + (x(1) + x(2)^2 - 7)^2;\n",
    "himmelblau_grad = @(x) [4*x(1)*(x(1)^2 + x(2) - 11) + 2*(x(1) + x(2)^2 - 7); ...\n",
    "                       2*(x(1)^2 + x(2) - 11) + 4*x(2)*(x(1) + x(2)^2 - 7)];\n",
    "\n",
    "fprintf('Test functions: Rosenbrock and Himmelblau\\n');\n",
    "\n",
    "% Line search function using Armijo condition\n",
    "function alpha_opt = armijo_line_search(func, grad, x, p, alpha_init, c1, rho)\n",
    "    % Armijo line search\n",
    "    % Input: func - objective, grad - gradient, x - current point, p - search direction\n",
    "    % Output: alpha_opt - step size\n",
    "    \n",
    "    alpha = alpha_init;\n",
    "    f0 = func(x);\n",
    "    g0 = grad(x);\n",
    "    phi0 = g0' * p;\n",
    "    \n",
    "    % Check descent direction\n",
    "    if phi0 >= 0\n",
    "        alpha_opt = 0;\n",
    "        return;\n",
    "    end\n",
    "    \n",
    "    while func(x + alpha * p) > f0 + c1 * alpha * phi0\n",
    "        alpha = rho * alpha;\n",
    "        if alpha < 1e-12\n",
    "            break;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    alpha_opt = alpha;\n",
    "end\n",
    "\n",
    "% Gradient Descent with line search\n",
    "fprintf('\\n1. Gradient Descent with Line Search:\\n');\n",
    "\n",
    "function [x_opt, f_opt, iterations] = gradient_descent(func, grad, x0, tol, max_iter)\n",
    "    % Gradient descent optimization with Armijo line search\n",
    "    % Input: func - objective function, grad - gradient, x0 - initial point\n",
    "    % Output: x_opt - optimal point, f_opt - optimal value, iterations\n",
    "    \n",
    "    x = x0;\n",
    "    \n",
    "    for iter = 1:max_iter\n",
    "        g = grad(x);\n",
    "        \n",
    "        % Check convergence\n",
    "        if norm(g) < tol\n",
    "            x_opt = x;\n",
    "            f_opt = func(x);\n",
    "            iterations = iter;\n",
    "            return;\n",
    "        end\n",
    "        \n",
    "        % Search direction (steepest descent)\n",
    "        p = -g;\n",
    "        \n",
    "        % Line search\n",
    "        alpha = armijo_line_search(func, grad, x, p, 1.0, 1e-4, 0.5);\n",
    "        \n",
    "        % Update\n",
    "        x = x + alpha * p;\n",
    "    end\n",
    "    \n",
    "    x_opt = x;\n",
    "    f_opt = func(x);\n",
    "    iterations = max_iter;\n",
    "end\n",
    "\n",
    "% Test gradient descent on Rosenbrock\n",
    "[x_gd, f_gd, iter_gd] = gradient_descent(rosenbrock, rosenbrock_grad, [-1.2; 1.0], 1e-6, 10000);\n",
    "fprintf('   Rosenbrock function, x0 = [-1.2, 1.0]:\\n');\n",
    "fprintf('   Optimal point: [%.6f, %.6f]\\n', x_gd(1), x_gd(2));\n",
    "fprintf('   Optimal value: %.8f (iterations: %d)\\n', f_gd, iter_gd);\n",
    "fprintf('   True optimum: [1, 1], f = 0\\n');\n",
    "\n",
    "% Newton's Method for Optimization\n",
    "fprintf('\\n2. Newton''s Method for Optimization:\\n');\n",
    "\n",
    "function [x_opt, f_opt, iterations] = newton_optimization(func, grad, hess, x0, tol, max_iter)\n",
    "    % Newton's method for optimization\n",
    "    % Input: func, grad, hess - function, gradient, Hessian, x0 - initial point\n",
    "    % Output: x_opt, f_opt, iterations\n",
    "    \n",
    "    x = x0;\n",
    "    \n",
    "    for iter = 1:max_iter\n",
    "        g = grad(x);\n",
    "        H = hess(x);\n",
    "        \n",
    "        if norm(g) < tol\n",
    "            x_opt = x;\n",
    "            f_opt = func(x);\n",
    "            iterations = iter;\n",
    "            return;\n",
    "        end\n",
    "        \n",
    "        % Check positive definiteness and modify if needed\n",
    "        [L, flag] = chol(H, 'lower');\n",
    "        if flag ~= 0\n",
    "            % Add regularization for non-positive definite Hessian\n",
    "            lambda = 1e-3;\n",
    "            H = H + lambda * eye(size(H));\n",
    "        end\n",
    "        \n",
    "        % Newton update: x = x - H^(-1) * g\n",
    "        delta_x = H \\ g;\n",
    "        \n",
    "        % Line search for robustness\n",
    "        alpha = armijo_line_search(func, grad, x, -delta_x, 1.0, 1e-4, 0.5);\n",
    "        x = x - alpha * delta_x;\n",
    "        \n",
    "        if norm(alpha * delta_x) < tol\n",
    "            x_opt = x;\n",
    "            f_opt = func(x);\n",
    "            iterations = iter;\n",
    "            return;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    x_opt = x;\n",
    "    f_opt = func(x);\n",
    "    iterations = max_iter;\n",
    "end\n",
    "\n",
    "% Rosenbrock Hessian\n",
    "rosenbrock_hess = @(x) [-400*(x(2) - 3*x(1)^2) + 2, -400*x(1); -400*x(1), 200];\n",
    "\n",
    "[x_newton, f_newton, iter_newton] = newton_optimization(rosenbrock, rosenbrock_grad, ...\n",
    "                                                       rosenbrock_hess, [-1.2; 1.0], 1e-8, 100);\n",
    "fprintf('   Newton''s method on Rosenbrock:\\n');\n",
    "fprintf('   Optimal point: [%.8f, %.8f]\\n', x_newton(1), x_newton(2));\n",
    "fprintf('   Optimal value: %.10f (iterations: %d)\\n', f_newton, iter_newton);\n",
    "\n",
    "% BFGS Quasi-Newton Method\n",
    "fprintf('\\n3. BFGS Quasi-Newton Method:\\n');\n",
    "\n",
    "function [x_opt, f_opt, iterations] = bfgs_method(func, grad, x0, tol, max_iter)\n",
    "    % BFGS quasi-Newton method\n",
    "    % Input: func, grad - function and gradient, x0 - initial point\n",
    "    % Output: x_opt, f_opt, iterations\n",
    "    \n",
    "    x = x0;\n",
    "    n = length(x0);\n",
    "    H = eye(n);  % Initial Hessian approximation\n",
    "    \n",
    "    for iter = 1:max_iter\n",
    "        g = grad(x);\n",
    "        \n",
    "        if norm(g) < tol\n",
    "            x_opt = x;\n",
    "            f_opt = func(x);\n",
    "            iterations = iter;\n",
    "            return;\n",
    "        end\n",
    "        \n",
    "        % Search direction\n",
    "        p = -H * g;\n",
    "        \n",
    "        % Line search\n",
    "        alpha = armijo_line_search(func, grad, x, p, 1.0, 1e-4, 0.5);\n",
    "        \n",
    "        % Update\n",
    "        x_new = x + alpha * p;\n",
    "        g_new = grad(x_new);\n",
    "        \n",
    "        % BFGS update\n",
    "        s = x_new - x;\n",
    "        y = g_new - g;\n",
    "        \n",
    "        % Check curvature condition\n",
    "        if abs(y' * s) > eps && norm(s) > eps\n",
    "            rho = 1 / (y' * s);\n",
    "            H = (eye(n) - rho * s * y') * H * (eye(n) - rho * y * s') + rho * s * s';\n",
    "        end\n",
    "        \n",
    "        x = x_new;\n",
    "    end\n",
    "    \n",
    "    x_opt = x;\n",
    "    f_opt = func(x);\n",
    "    iterations = max_iter;\n",
    "end\n",
    "\n",
    "[x_bfgs, f_bfgs, iter_bfgs] = bfgs_method(rosenbrock, rosenbrock_grad, [-1.2; 1.0], 1e-8, 1000);\n",
    "fprintf('   BFGS method on Rosenbrock:\\n');\n",
    "fprintf('   Optimal point: [%.8f, %.8f]\\n', x_bfgs(1), x_bfgs(2));\n",
    "fprintf('   Optimal value: %.10f (iterations: %d)\\n', f_bfgs, iter_bfgs);\n",
    "\n",
    "% Method comparison\n",
    "fprintf('\\n4. Method Comparison:\\n');\n",
    "fprintf('   Method           | Iterations | Final Value     | Error\\n');\n",
    "fprintf('   -----------------|------------|-----------------|----------\\n');\n",
    "fprintf('   Gradient Descent | %8d   | %13.8f | %.2e\\n', iter_gd, f_gd, norm(x_gd - [1;1]));\n",
    "fprintf('   Newton           | %8d   | %13.10f | %.2e\\n', iter_newton, f_newton, norm(x_newton - [1;1]));\n",
    "fprintf('   BFGS             | %8d   | %13.10f | %.2e\\n', iter_bfgs, f_bfgs, norm(x_bfgs - [1;1]));\n",
    "```\n",
    "\n",
    "## 4. Constrained Optimization\n",
    "\n",
    "```octave\n",
    "% Constrained optimization methods\n",
    "fprintf('\\n=== Constrained Optimization ===\\n');\n",
    "\n",
    "% Problem: minimize f(x,y) = (x-2)² + (y-1)² subject to x² + y² ≤ 1\n",
    "objective = @(x) (x(1) - 2)^2 + (x(2) - 1)^2;\n",
    "objective_grad = @(x) [2*(x(1) - 2); 2*(x(2) - 1)];\n",
    "\n",
    "% Constraint: g(x,y) = x² + y² - 1 ≤ 0\n",
    "constraint = @(x) x(1)^2 + x(2)^2 - 1;\n",
    "constraint_grad = @(x) [2*x(1); 2*x(2)];\n",
    "\n",
    "fprintf('Problem: minimize (x-2)² + (y-1)² subject to x² + y² ≤ 1\\n');\n",
    "\n",
    "% Penalty Method\n",
    "fprintf('\\n1. Penalty Method:\\n');\n",
    "\n",
    "function [x_opt, f_opt, iterations] = penalty_method(func, grad_func, constraint_func, constraint_grad_func, x0, tol)\n",
    "    % Penalty method for constrained optimization\n",
    "    % Input: func, grad_func - objective, constraint_func, constraint_grad_func - constraint\n",
    "    % Output: x_opt, f_opt, iterations\n",
    "    \n",
    "    penalty_param = 1;\n",
    "    max_penalty_iter = 15;\n",
    "    \n",
    "    x_current = x0;\n",
    "    \n",
    "    for penalty_iter = 1:max_penalty_iter\n",
    "        % Create penalized objective and gradient\n",
    "        penalized_func = @(x) func(x) + penalty_param * max(0, constraint_func(x))^2;\n",
    "        \n",
    "        penalized_grad = @(x) penalty_gradient_func(x, grad_func, constraint_func, constraint_grad_func, penalty_param);\n",
    "        \n",
    "        % Solve unconstrained problem\n",
    "        [x_opt, ~, ~] = bfgs_method(penalized_func, penalized_grad, x_current, tol, 1000);\n",
    "        \n",
    "        % Check constraint satisfaction\n",
    "        constraint_violation = max(0, constraint_func(x_opt));\n",
    "        \n",
    "        if constraint_violation < tol\n",
    "            f_opt = func(x_opt);\n",
    "            iterations = penalty_iter;\n",
    "            return;\n",
    "        end\n",
    "        \n",
    "        % Increase penalty parameter\n",
    "        penalty_param = penalty_param * 10;\n",
    "        x_current = x_opt;  % Warm start\n",
    "        \n",
    "        % Print progress\n",
    "        fprintf('     Penalty iteration %d: violation = %.2e, penalty = %.1e\\n', ...\n",
    "                penalty_iter, constraint_violation, penalty_param);\n",
    "    end\n",
    "    \n",
    "    f_opt = func(x_opt);\n",
    "    iterations = max_penalty_iter;\n",
    "end\n",
    "\n",
    "function g_pen = penalty_gradient_func(x, grad_func, constraint_func, constraint_grad_func, penalty_param)\n",
    "    % Gradient of penalized objective\n",
    "    g_pen = grad_func(x);\n",
    "    constraint_val = constraint_func(x);\n",
    "    \n",
    "    if constraint_val > 0\n",
    "        g_pen = g_pen + 2 * penalty_param * constraint_val * constraint_grad_func(x);\n",
    "    end\n",
    "end\n",
    "\n",
    "% Apply penalty method\n",
    "[x_penalty, f_penalty, iter_penalty] = penalty_method(objective, objective_grad, ...\n",
    "                                                     constraint, constraint_grad, [0.5; 0.5], 1e-8);\n",
    "\n",
    "fprintf('   Penalty method result:\\n');\n",
    "fprintf('   Optimal point: [%.8f, %.8f]\\n', x_penalty(1), x_penalty(2));\n",
    "fprintf('   Optimal value: %.10f (iterations: %d)\\n', f_penalty, iter_penalty);\n",
    "fprintf('   Constraint value: %.8f (should be ≤ 0)\\n', constraint(x_penalty));\n",
    "\n",
    "% Analytical solution for comparison\n",
    "% The constrained optimum lies on the boundary at the point closest to (2,1)\n",
    "% This is at (2/√5, 1/√5) ≈ (0.8944, 0.4472)\n",
    "target = [2; 1];\n",
    "x_analytical = target / norm(target);\n",
    "f_analytical = objective(x_analytical);\n",
    "\n",
    "fprintf('   Analytical solution: [%.8f, %.8f]\\n', x_analytical(1), x_analytical(2));\n",
    "fprintf('   Analytical value: %.10f\\n', f_analytical);\n",
    "fprintf('   Error: %.8f\\n', norm(x_penalty - x_analytical));\n",
    "\n",
    "% Lagrange Multipliers (KKT conditions)\n",
    "fprintf('\\n2. Lagrange Multipliers Analysis:\\n');\n",
    "\n",
    "function [x_opt, lambda_opt, iterations] = lagrange_newton(func, func_grad, constraint_func, constraint_grad_func, x0, tol, max_iter)\n",
    "    % Newton's method for Lagrange multiplier system\n",
    "    % Input: func, func_grad - objective, constraint_func, constraint_grad_func - constraint\n",
    "    % Output: x_opt - optimal point, lambda_opt - multiplier, iterations\n",
    "    \n",
    "    % Initialize [x; lambda]\n",
    "    z = [x0; 1.0];\n",
    "    \n",
    "    for iter = 1:max_iter\n",
    "        x = z(1:2);\n",
    "        lambda = z(3);\n",
    "        \n",
    "        % KKT system: ∇f + λ∇g = 0, g = 0\n",
    "        grad_f = func_grad(x);\n",
    "        grad_g = constraint_grad_func(x);\n",
    "        g_val = constraint_func(x);\n",
    "        \n",
    "        F = [grad_f + lambda * grad_g; g_val];\n",
    "        \n",
    "        if norm(F) < tol\n",
    "            x_opt = x;\n",
    "            lambda_opt = lambda;\n",
    "            iterations = iter;\n",
    "            return;\n",
    "        end\n",
    "        \n",
    "        % Jacobian of KKT system\n",
    "        % Hessian of objective: ∇²f = [2, 0; 0, 2]\n",
    "        % Hessian of constraint: ∇²g = [2, 0; 0, 2]\n",
    "        hess_f = [2, 0; 0, 2];\n",
    "        hess_g = [2, 0; 0, 2];\n",
    "        \n",
    "        KKT_jacobian = [hess_f + lambda * hess_g, grad_g; ...\n",
    "                       grad_g', 0];\n",
    "        \n",
    "        % Newton update\n",
    "        delta_z = KKT_jacobian \\ (-F);\n",
    "        z = z + delta_z;\n",
    "        \n",
    "        if norm(delta_z) < tol\n",
    "            x_opt = z(1:2);\n",
    "            lambda_opt = z(3);\n",
    "            iterations = iter;\n",
    "            return;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    x_opt = z(1:2);\n",
    "    lambda_opt = z(3);\n",
    "    iterations = max_iter;\n",
    "end\n",
    "\n",
    "[x_lagrange, lambda_opt, iter_lagrange] = lagrange_newton(objective, objective_grad, ...\n",
    "                                                         constraint, constraint_grad, [0.8; 0.4], 1e-12, 100);\n",
    "\n",
    "fprintf('   Lagrange multiplier method:\\n');\n",
    "fprintf('   Optimal point: [%.10f, %.10f]\\n', x_lagrange(1), x_lagrange(2));\n",
    "fprintf('   Optimal value: %.12f\\n', objective(x_lagrange));\n",
    "fprintf('   Lagrange multiplier λ: %.8f\\n', lambda_opt);\n",
    "fprintf('   Constraint satisfaction: %.2e\\n', abs(constraint(x_lagrange)));\n",
    "fprintf('   Iterations: %d\\n', iter_lagrange);\n",
    "\n",
    "% Projected Gradient Method\n",
    "fprintf('\\n3. Projected Gradient Method:\\n');\n",
    "\n",
    "function [x_opt, f_opt, iterations] = projected_gradient(func, grad_func, constraint_func, x0, tol, max_iter)\n",
    "    % Projected gradient method for inequality constraints\n",
    "    % Input: func, grad_func - objective, constraint_func - constraint, x0 - initial point\n",
    "    % Output: x_opt, f_opt, iterations\n",
    "    \n",
    "    x = x0;\n",
    "    \n",
    "    % Ensure initial point is feasible\n",
    "    if constraint_func(x) > 0\n",
    "        % Simple projection onto unit circle\n",
    "        x = x / norm(x) * 0.99;  % Slightly inside boundary\n",
    "    end\n",
    "    \n",
    "    for iter = 1:max_iter\n",
    "        g = grad_func(x);\n",
    "        \n",
    "        if norm(g) < tol\n",
    "            x_opt = x;\n",
    "            f_opt = func(x);\n",
    "            iterations = iter;\n",
    "            return;\n",
    "        end\n",
    "        \n",
    "        % Take gradient step\n",
    "        alpha = 0.01;  % Small step size for stability\n",
    "        x_trial = x - alpha * g;\n",
    "        \n",
    "        % Project onto feasible region if necessary\n",
    "        if constraint_func(x_trial) > 0\n",
    "            % Project onto unit circle\n",
    "            x_trial = x_trial / norm(x_trial);\n",
    "        end\n",
    "        \n",
    "        % Check for progress\n",
    "        if func(x_trial) < func(x) || norm(x_trial - x) < tol\n",
    "            x = x_trial;\n",
    "        else\n",
    "            % Reduce step size\n",
    "            alpha = alpha / 2;\n",
    "            if alpha < 1e-10\n",
    "                break;\n",
    "            end\n",
    "            x = x - alpha * g;\n",
    "            if constraint_func(x) > 0\n",
    "                x = x / norm(x);\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        % Convergence check\n",
    "        if norm(alpha * g) < tol\n",
    "            x_opt = x;\n",
    "            f_opt = func(x);\n",
    "            iterations = iter;\n",
    "            return;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    x_opt = x;\n",
    "    f_opt = func(x);\n",
    "    iterations = max_iter;\n",
    "end\n",
    "\n",
    "[x_projected, f_projected, iter_projected] = projected_gradient(objective, objective_grad, constraint, [0.5; 0.5], 1e-6, 5000);\n",
    "\n",
    "fprintf('   Projected gradient method:\\n');\n",
    "fprintf('   Optimal point: [%.8f, %.8f]\\n', x_projected(1), x_projected(2));\n",
    "fprintf('   Optimal value: %.10f (iterations: %d)\\n', f_projected, iter_projected);\n",
    "fprintf('   Constraint satisfaction: %.8f\\n', constraint(x_projected));\n",
    "fprintf('   Error from analytical: %.8f\\n', norm(x_projected - x_analytical));\n",
    "```\n",
    "\n",
    "## 5. Global Optimization and Advanced Methods\n",
    "\n",
    "```octave\n",
    "% Global optimization techniques\n",
    "fprintf('\\n=== Global Optimization ===\\n');\n",
    "\n",
    "% Multi-modal test function: Rastrigin function\n",
    "rastrigin_2d = @(x) 20 + x(1)^2 + x(2)^2 - 10*(cos(2*pi*x(1)) + cos(2*pi*x(2)));\n",
    "\n",
    "fprintf('Test function: 2D Rastrigin (highly multi-modal)\\n');\n",
    "fprintf('Global optimum: [0, 0], f = 0\\n');\n",
    "\n",
    "% Simulated Annealing\n",
    "fprintf('\\n1. Simulated Annealing:\\n');\n",
    "\n",
    "function [x_best, f_best, iterations] = simulated_annealing(func, x0, bounds, T0, cooling_rate, max_iter)\n",
    "    % Simulated annealing global optimization\n",
    "    % Input: func - objective, x0 - initial point, bounds - [lower, upper], T0 - initial temperature\n",
    "    % Output: x_best, f_best, iterations\n",
    "    \n",
    "    x_current = x0;\n",
    "    f_current = func(x_current);\n",
    "    \n",
    "    x_best = x_current;\n",
    "    f_best = f_current;\n",
    "    \n",
    "    T = T0;\n",
    "    accepted = 0;\n",
    "    \n",
    "    for iter = 1:max_iter\n",
    "        % Generate neighbor\n",
    "        step_size = min(1.0, T / T0);  % Adaptive step size\n",
    "        x_new = x_current + step_size * randn(size(x_current));\n",
    "        \n",
    "        % Enforce bounds\n",
    "        x_new = max(bounds(1), min(bounds(2), x_new));\n",
    "        \n",
    "        f_new = func(x_new);\n",
    "        \n",
    "        % Accept or reject\n",
    "        delta_f = f_new - f_current;\n",
    "        \n",
    "        if delta_f < 0 || (T > eps && rand() < exp(-delta_f / T))\n",
    "            x_current = x_new;\n",
    "            f_current = f_new;\n",
    "            accepted = accepted + 1;\n",
    "            \n",
    "            % Update best\n",
    "            if f_current < f_best\n",
    "                x_best = x_current;\n",
    "                f_best = f_current;\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        % Cool down\n",
    "        T = T * cooling_rate;\n",
    "        \n",
    "        % Print progress every 1000 iterations\n",
    "        if mod(iter, 1000) == 0\n",
    "            fprintf('     Iteration %d: T = %.2e, Best = %.6f, Accept rate = %.2f%%\\n', ...\n",
    "                    iter, T, f_best, 100 * accepted / iter);\n",
    "        end\n",
    "        \n",
    "        if T < 1e-12\n",
    "            iterations = iter;\n",
    "            return;\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    iterations = max_iter;\n",
    "end\n",
    "\n",
    "% Test simulated annealing on Rastrigin function\n",
    "[x_sa, f_sa, iter_sa] = simulated_annealing(rastrigin_2d, [2.5; -3.1], [-5, 5], 10.0, 0.995, 50000);\n",
    "\n",
    "fprintf('   Simulated Annealing on 2D Rastrigin:\\n');\n",
    "fprintf('   Best point: [%.6f, %.6f]\\n', x_sa(1), x_sa(2));\n",
    "fprintf('   Best value: %.8f (iterations: %d)\\n', f_sa, iter_sa);\n",
    "fprintf('   Distance from global optimum: %.6f\\n', norm(x_sa));\n",
    "\n",
    "% Genetic Algorithm\n",
    "fprintf('\\n2. Genetic Algorithm:\\n');\n",
    "\n",
    "function [x_best, f_best] = genetic_algorithm(func, bounds, pop_size, generations)\n",
    "    % Genetic algorithm for global optimization\n",
    "    % Input: func - objective, bounds - [lower, upper], pop_size, generations\n",
    "    % Output: x_best, f_best\n",
    "    \n",
    "    dim = 2;  % 2D problem\n",
    "    \n",
    "    % Initialize population\n",
    "    population = bounds(1) + (bounds(2) - bounds(1)) * rand(pop_size, dim);\n",
    "    \n",
    "    f_best = inf;\n",
    "    \n",
    "    for gen = 1:generations\n",
    "        % Evaluate fitness\n",
    "        fitness = zeros(pop_size, 1);\n",
    "        for i = 1:pop_size\n",
    "            fitness(i) = func(population(i, :)');\n",
    "        end\n",
    "        \n",
    "        % Find best in current generation\n",
    "        [f_best_gen, best_idx] = min(fitness);\n",
    "        x_best_gen = population(best_idx, :)';\n",
    "        \n",
    "        if f_best_gen < f_best\n",
    "            x_best = x_best_gen;\n",
    "            f_best = f_best_gen;\n",
    "        end\n",
    "        \n",
    "        % Selection, crossover, and mutation\n",
    "        new_population = zeros(size(population));\n",
    "        \n",
    "        % Keep best individual (elitism)\n",
    "        new_population(1, :) = population(best_idx, :);\n",
    "        \n",
    "        for i = 2:pop_size\n",
    "            % Tournament selection\n",
    "            tournament_size = 5;\n",
    "            tournament_idx = randperm(pop_size, tournament_size);\n",
    "            tournament_fitness = fitness(tournament_idx);\n",
    "            [~, winner_idx] = min(tournament_fitness);\n",
    "            parent1 = population(tournament_idx(winner_idx), :);\n",
    "            \n",
    "            % Second parent\n",
    "            tournament_idx = randperm(pop_size, tournament_size);\n",
    "            tournament_fitness = fitness(tournament_idx);\n",
    "            [~, winner_idx] = min(tournament_fitness);\n",
    "            parent2 = population(tournament_idx(winner_idx), :);\n",
    "            \n",
    "            % Crossover (blend)\n",
    "            alpha = 0.5;\n",
    "            child = alpha * parent1 + (1 - alpha) * parent2;\n",
    "            \n",
    "            % Mutation\n",
    "            mutation_rate = 0.1;\n",
    "            mutation_strength = 0.5;\n",
    "            if rand() < mutation_rate\n",
    "                child = child + mutation_strength * randn(size(child));\n",
    "            end\n",
    "            \n",
    "            % Enforce bounds\n",
    "            child = max(bounds(1), min(bounds(2), child));\n",
    "            \n",
    "            new_population(i, :) = child;\n",
    "        end\n",
    "        \n",
    "        population = new_population;\n",
    "        \n",
    "        % Print progress every 50 generations\n",
    "        if mod(gen, 50) == 0\n",
    "            fprintf('     Generation %d: Best fitness = %.6f\\n', gen, f_best);\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "[x_ga, f_ga] = genetic_algorithm(rastrigin_2d, [-5, 5], 100, 500);\n",
    "\n",
    "fprintf('   Genetic Algorithm on 2D Rastrigin:\\n');\n",
    "fprintf('   Best point: [%.6f, %.6f]\\n', x_ga(1), x_ga(2));\n",
    "fprintf('   Best value: %.8f\\n', f_ga);\n",
    "fprintf('   Distance from global optimum: %.6f\\n', norm(x_ga));\n",
    "\n",
    "% Multi-start optimization\n",
    "fprintf('\\n3. Multi-Start Local Optimization:\\n');\n",
    "\n",
    "function [x_best, f_best, success_rate] = multistart_optimization(func, grad_func, bounds, n_starts)\n",
    "    % Multi-start local optimization\n",
    "    % Input: func, grad_func - objective and gradient, bounds, n_starts\n",
    "    % Output: x_best, f_best, success_rate\n",
    "    \n",
    "    dim = 2;\n",
    "    f_best = inf;\n",
    "    x_best = [];\n",
    "    \n",
    "    local_minima = [];\n",
    "    successful_runs = 0;\n",
    "    \n",
    "    for start = 1:n_starts\n",
    "        % Random starting point\n",
    "        x0 = bounds(1) + (bounds(2) - bounds(1)) * rand(dim, 1);\n",
    "        \n",
    "        try\n",
    "            [x_local, f_local, ~] = bfgs_method(func, grad_func, x0, 1e-8, 1000);\n",
    "            \n",
    "            % Check if optimization was successful\n",
    "            if ~isnan(f_local) && ~isinf(f_local)\n",
    "                successful_runs = successful_runs + 1;\n",
    "                \n",
    "                % Check if this is a new local minimum\n",
    "                is_new = true;\n",
    "                for i = 1:size(local_minima, 2)\n",
    "                    if norm(x_local - local_minima(:, i)) < 0.2\n",
    "                        is_new = false;\n",
    "                        break;\n",
    "                    end\n",
    "                end\n",
    "                \n",
    "                if is_new\n",
    "                    local_minima = [local_minima, x_local];\n",
    "                end\n",
    "                \n",
    "                if f_local < f_best\n",
    "                    f_best = f_local;\n",
    "                    x_best = x_local;\n",
    "                end\n",
    "            end\n",
    "        catch\n",
    "            % Optimization failed - continue\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    success_rate = successful_runs / n_starts;\n",
    "    fprintf('     Found %d distinct local minima from %d successful runs\\n', ...\n",
    "            size(local_minima, 2), successful_runs);\n",
    "end\n",
    "\n",
    "% Test multi-start on Himmelblau function (has 4 global minima)\n",
    "himmel_2d = @(x) himmelblau(x);\n",
    "himmel_grad_2d = @(x) himmelblau_grad(x);\n",
    "\n",
    "[x_multi, f_multi, success] = multistart_optimization(himmel_2d, himmel_grad_2d, [-5, 5], 50);\n",
    "\n",
    "fprintf('   Multi-start on Himmelblau function:\\n');\n",
    "fprintf('   Best point: [%.6f, %.6f]\\n', x_multi(1), x_multi(2));\n",
    "fprintf('   Best value: %.8f\\n', f_multi);\n",
    "fprintf('   Success rate: %.1f%%\\n', success * 100);\n",
    "\n",
    "% Known global minima of Himmelblau: (3,2), (-2.805118, 3.131312), (-3.779310, -3.283186), (3.584428, -1.848126)\n",
    "known_minima = [3, 2; -2.805118, 3.131312; -3.779310, -3.283186; 3.584428, -1.848126];\n",
    "min_distance = inf;\n",
    "closest_minimum = [];\n",
    "for i = 1:size(known_minima, 1)\n",
    "    distance = norm(x_multi - known_minima(i, :)');\n",
    "    if distance < min_distance\n",
    "        min_distance = distance;\n",
    "        closest_minimum = known_minima(i, :);\n",
    "    end\n",
    "end\n",
    "\n",
    "fprintf('   Distance to nearest known minimum [%.3f, %.3f]: %.6f\\n', ...\n",
    "        closest_minimum(1), closest_minimum(2), min_distance);\n",
    "\n",
    "% Differential Evolution\n",
    "fprintf('\\n4. Differential Evolution:\\n');\n",
    "\n",
    "function [x_best, f_best] = differential_evolution(func, bounds, pop_size, generations, F, CR)\n",
    "    % Differential Evolution algorithm\n",
    "    % Input: func - objective, bounds, pop_size, generations, F - mutation factor, CR - crossover rate\n",
    "    % Output: x_best, f_best\n",
    "    \n",
    "    dim = 2;\n",
    "    \n",
    "    % Initialize population\n",
    "    population = bounds(1) + (bounds(2) - bounds(1)) * rand(pop_size, dim);\n",
    "    \n",
    "    % Evaluate initial population\n",
    "    fitness = zeros(pop_size, 1);\n",
    "    for i = 1:pop_size\n",
    "        fitness(i) = func(population(i, :)');\n",
    "    end\n",
    "    \n",
    "    [f_best, best_idx] = min(fitness);\n",
    "    x_best = population(best_idx, :)';\n",
    "    \n",
    "    for gen = 1:generations\n",
    "        for i = 1:pop_size\n",
    "            % Select three random individuals (different from current)\n",
    "            candidates = setdiff(1:pop_size, i);\n",
    "            selected = candidates(randperm(length(candidates), 3));\n",
    "            a = selected(1); b = selected(2); c = selected(3);\n",
    "            \n",
    "            % Mutation: v = xa + F * (xb - xc)\n",
    "            mutant = population(a, :) + F * (population(b, :) - population(c, :));\n",
    "            \n",
    "            % Ensure bounds\n",
    "            mutant = max(bounds(1), min(bounds(2), mutant));\n",
    "            \n",
    "            % Crossover\n",
    "            trial = population(i, :);\n",
    "            for j = 1:dim\n",
    "                if rand() < CR || j == randi(dim)  % Ensure at least one component from mutant\n",
    "                    trial(j) = mutant(j);\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            % Selection\n",
    "            f_trial = func(trial');\n",
    "            if f_trial < fitness(i)\n",
    "                population(i, :) = trial;\n",
    "                fitness(i) = f_trial;\n",
    "                \n",
    "                if f_trial < f_best\n",
    "                    x_best = trial';\n",
    "                    f_best = f_trial;\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        % Print progress\n",
    "        if mod(gen, 100) == 0\n",
    "            fprintf('     Generation %d: Best fitness = %.6f\\n', gen, f_best);\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "[x_de, f_de] = differential_evolution(rastrigin_2d, [-5, 5], 50, 1000, 0.5, 0.7);\n",
    "\n",
    "fprintf('   Differential Evolution on 2D Rastrigin:\\n');\n",
    "fprintf('   Best point: [%.6f, %.6f]\\n', x_de(1), x_de(2));\n",
    "fprintf('   Best value: %.8f\\n', f_de);\n",
    "fprintf('   Distance from global optimum: %.6f\\n', norm(x_de));\n",
    "\n",
    "% Global optimization comparison\n",
    "fprintf('\\n5. Global Method Comparison on Rastrigin:\\n');\n",
    "fprintf('   Method               | Best Value | Distance to Global | Success\\n');\n",
    "fprintf('   ---------------------|------------|-------------------|--------\\n');\n",
    "fprintf('   Simulated Annealing  | %8.4f   | %15.6f   | %s\\n', f_sa, norm(x_sa), f_sa < 1 ? 'Yes' : 'No');\n",
    "fprintf('   Genetic Algorithm    | %8.4f   | %15.6f   | %s\\n', f_ga, norm(x_ga), f_ga < 1 ? 'Yes' : 'No');\n",
    "fprintf('   Differential Evolution| %8.4f   | %15.6f   | %s\\n', f_de, norm(x_de), f_de < 1 ? 'Yes' : 'No');\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Advanced Convergence Analysis and Visualization\n",
    "\n",
    "```octave\n",
    "% Advanced convergence analysis and method comparison\n",
    "fprintf('\\n=== Advanced Convergence Analysis ===\\n');\n",
    "\n",
    "% Convergence rate analysis for different methods\n",
    "fprintf('\\n1. Theoretical vs Actual Convergence Rates:\\n');\n",
    "\n",
    "% Test all root-finding methods on the same function\n",
    "test_func = @(x) x^3 - 2*x - 5;  % Root approximately at x = 2.094551\n",
    "test_func_prime = @(x) 3*x^2 - 2;\n",
    "true_root = 2.0945514815423265;  % High-precision root\n",
    "\n",
    "fprintf('   Testing function: f(x) = x³ - 2x - 5\\n');\n",
    "fprintf('   True root: %.10f\\n\\n', true_root);\n",
    "\n",
    "% Bisection method detailed analysis\n",
    "[root_bis, iter_bis, errors_bis] = bisection(test_func, 1.5, 2.5, 1e-12, 50);\n",
    "actual_errors_bis = abs(true_root - root_bis);\n",
    "\n",
    "% Newton-Raphson detailed analysis  \n",
    "[root_nr, iter_nr, errors_nr] = newton_raphson(test_func, test_func_prime, 2.5, 1e-12, 20);\n",
    "actual_errors_nr = abs(true_root - root_nr);\n",
    "\n",
    "% Secant method detailed analysis\n",
    "[root_sec, iter_sec, errors_sec] = secant_method(test_func, 1.8, 2.3, 1e-12, 30);\n",
    "actual_errors_sec = abs(true_root - root_sec);\n",
    "\n",
    "% Display detailed convergence comparison\n",
    "fprintf('   Detailed Method Comparison:\\n');\n",
    "fprintf('   ================================================================\\n');\n",
    "fprintf('   Method         | Iterations | Final Error  | Convergence Rate\\n');\n",
    "fprintf('   ================================================================\\n');\n",
    "\n",
    "% Calculate convergence rates\n",
    "if length(errors_bis) >= 3\n",
    "    q_bis = log(errors_bis(end)/errors_bis(end-1)) / log(errors_bis(end-1)/errors_bis(end-2));\n",
    "else\n",
    "    q_bis = 1.0;\n",
    "end\n",
    "\n",
    "if length(errors_nr) >= 3\n",
    "    q_nr = log(errors_nr(end)/errors_nr(end-1)) / log(errors_nr(end-1)/errors_nr(end-2));\n",
    "else\n",
    "    q_nr = 2.0;\n",
    "end\n",
    "\n",
    "if length(errors_sec) >= 3\n",
    "    q_sec = log(errors_sec(end)/errors_sec(end-1)) / log(errors_sec(end-1)/errors_sec(end-2));\n",
    "else\n",
    "    q_sec = 1.618;\n",
    "end\n",
    "\n",
    "fprintf('   Bisection      | %8d   | %10.2e   | %8.2f (Linear)\\n', iter_bis, actual_errors_bis, q_bis);\n",
    "fprintf('   Newton-Raphson | %8d   | %10.2e   | %8.2f (Quadratic)\\n', iter_nr, actual_errors_nr, q_nr);\n",
    "fprintf('   Secant         | %8d   | %10.2e   | %8.2f (Superlinear)\\n', iter_sec, actual_errors_sec, q_sec);\n",
    "fprintf('   ================================================================\\n');\n",
    "\n",
    "% Work-precision diagram analysis\n",
    "fprintf('\\n2. Work-Precision Analysis:\\n');\n",
    "\n",
    "tolerances = [1e-4, 1e-6, 1e-8, 1e-10, 1e-12];\n",
    "methods = {'Bisection', 'Newton-Raphson', 'Secant'};\n",
    "\n",
    "fprintf('   Tolerance   | Bisection Iter | Newton Iter | Secant Iter\\n');\n",
    "fprintf('   ------------|----------------|-------------|------------\\n');\n",
    "\n",
    "for i = 1:length(tolerances)\n",
    "    tol = tolerances(i);\n",
    "    \n",
    "    try\n",
    "        [~, iter_b, ~] = bisection(test_func, 1.5, 2.5, tol, 100);\n",
    "    catch\n",
    "        iter_b = NaN;\n",
    "    end\n",
    "    \n",
    "    try\n",
    "        [~, iter_n, ~] = newton_raphson(test_func, test_func_prime, 2.5, tol, 50);\n",
    "    catch\n",
    "        iter_n = NaN;\n",
    "    end\n",
    "    \n",
    "    try\n",
    "        [~, iter_s, ~] = secant_method(test_func, 1.8, 2.3, tol, 50);\n",
    "    catch\n",
    "        iter_s = NaN;\n",
    "    end\n",
    "    \n",
    "    fprintf('   %8.0e   | %12d   | %9d   | %9d\\n', tol, iter_b, iter_n, iter_s);\n",
    "end\n",
    "\n",
    "% Robustness analysis\n",
    "fprintf('\\n3. Robustness Analysis:\\n');\n",
    "fprintf('   Testing methods with poor initial guesses:\\n\\n');\n",
    "\n",
    "poor_guesses = [0.5, 5.0, -1.0, 10.0];\n",
    "success_count = zeros(3, length(poor_guesses));\n",
    "\n",
    "for i = 1:length(poor_guesses)\n",
    "    guess = poor_guesses(i);\n",
    "    fprintf('   Initial guess: %.1f\\n', guess);\n",
    "    \n",
    "    % Bisection (needs bracket adjustment)\n",
    "    try\n",
    "        if test_func(guess) * test_func(guess + 1) < 0\n",
    "            [~, ~, ~] = bisection(test_func, guess, guess + 1, 1e-8, 50);\n",
    "            success_count(1, i) = 1;\n",
    "            fprintf('     Bisection: SUCCESS\\n');\n",
    "        else\n",
    "            fprintf('     Bisection: FAILED (no sign change)\\n');\n",
    "        end\n",
    "    catch\n",
    "        fprintf('     Bisection: FAILED (numerical error)\\n');\n",
    "    end\n",
    "    \n",
    "    % Newton-Raphson\n",
    "    try\n",
    "        [root_test, ~, ~] = newton_raphson(test_func, test_func_prime, guess, 1e-8, 50);\n",
    "        if abs(root_test - true_root) < 1e-6\n",
    "            success_count(2, i) = 1;\n",
    "            fprintf('     Newton-Raphson: SUCCESS\\n');\n",
    "        else\n",
    "            fprintf('     Newton-Raphson: CONVERGED TO WRONG ROOT\\n');\n",
    "        end\n",
    "    catch\n",
    "        fprintf('     Newton-Raphson: FAILED (numerical error)\\n');\n",
    "    end\n",
    "    \n",
    "    % Secant\n",
    "    try\n",
    "        [root_test, ~, ~] = secant_method(test_func, guess, guess + 0.1, 1e-8, 50);\n",
    "        if abs(root_test - true_root) < 1e-6\n",
    "            success_count(3, i) = 1;\n",
    "            fprintf('     Secant: SUCCESS\\n');\n",
    "        else\n",
    "            fprintf('     Secant: CONVERGED TO WRONG ROOT\\n');\n",
    "        end\n",
    "    catch\n",
    "        fprintf('     Secant: FAILED (numerical error)\\n');\n",
    "    end\n",
    "    fprintf('\\n');\n",
    "end\n",
    "\n",
    "% Overall success rates\n",
    "fprintf('   Overall Success Rates:\\n');\n",
    "fprintf('   Bisection: %.0f%% | Newton-Raphson: %.0f%% | Secant: %.0f%%\\n', ...\n",
    "        100*sum(success_count(1,:))/length(poor_guesses), ...\n",
    "        100*sum(success_count(2,:))/length(poor_guesses), ...\n",
    "        100*sum(success_count(3,:))/length(poor_guesses));\n",
    "```\n",
    "\n",
    "## 7. Optimization Method Benchmarking Suite\n",
    "\n",
    "```octave\n",
    "% Comprehensive optimization benchmarking\n",
    "fprintf('\\n=== Optimization Method Benchmarking ===\\n');\n",
    "\n",
    "% Define comprehensive test suite of optimization problems\n",
    "fprintf('1. Standard Test Function Suite:\\n');\n",
    "\n",
    "% Test function definitions\n",
    "test_functions = struct();\n",
    "\n",
    "% Sphere function (unimodal, separable)\n",
    "test_functions(1).name = 'Sphere';\n",
    "test_functions(1).func = @(x) sum(x.^2);\n",
    "test_functions(1).grad = @(x) 2*x;\n",
    "test_functions(1).optimum = [0; 0];\n",
    "test_functions(1).opt_value = 0;\n",
    "test_functions(1).init = [3; -2];\n",
    "\n",
    "% Rosenbrock function (unimodal, non-separable)\n",
    "test_functions(2).name = 'Rosenbrock';\n",
    "test_functions(2).func = rosenbrock;\n",
    "test_functions(2).grad = rosenbrock_grad;\n",
    "test_functions(2).optimum = [1; 1];\n",
    "test_functions(2).opt_value = 0;\n",
    "test_functions(2).init = [-1.2; 1.0];\n",
    "\n",
    "% Beale function (unimodal)\n",
    "beale_func = @(x) (1.5 - x(1) + x(1)*x(2))^2 + (2.25 - x(1) + x(1)*x(2)^2)^2 + (2.625 - x(1) + x(1)*x(2)^3)^2;\n",
    "beale_grad = @(x) [2*(1.5 - x(1) + x(1)*x(2))*(x(2) - 1) + 2*(2.25 - x(1) + x(1)*x(2)^2)*(x(2)^2 - 1) + 2*(2.625 - x(1) + x(1)*x(2)^3)*(x(2)^3 - 1); ...\n",
    "                   2*(1.5 - x(1) + x(1)*x(2))*x(1) + 2*(2.25 - x(1) + x(1)*x(2)^2)*2*x(1)*x(2) + 2*(2.625 - x(1) + x(1)*x(2)^3)*3*x(1)*x(2)^2];\n",
    "\n",
    "test_functions(3).name = 'Beale';\n",
    "test_functions(3).func = beale_func;\n",
    "test_functions(3).grad = beale_grad;\n",
    "test_functions(3).optimum = [3; 0.5];\n",
    "test_functions(3).opt_value = 0;\n",
    "test_functions(3).init = [1; 1];\n",
    "\n",
    "% Himmelblau function (multimodal)\n",
    "test_functions(4).name = 'Himmelblau';\n",
    "test_functions(4).func = himmelblau;\n",
    "test_functions(4).grad = himmelblau_grad;\n",
    "test_functions(4).optimum = [3; 2];  % One of four global minima\n",
    "test_functions(4).opt_value = 0;\n",
    "test_functions(4).init = [0; 0];\n",
    "\n",
    "% Benchmarking framework\n",
    "methods = {'Gradient Descent', 'Newton', 'BFGS'};\n",
    "results = struct();\n",
    "\n",
    "fprintf('\\n2. Performance Benchmarking Results:\\n');\n",
    "fprintf('=====================================================================================================================\\n');\n",
    "fprintf('Function     | Method           | Iterations | Final Value      | Error to Optimum | Time (Relative) | Status\\n');\n",
    "fprintf('=====================================================================================================================\\n');\n",
    "\n",
    "for f = 1:length(test_functions)\n",
    "    prob = test_functions(f);\n",
    "    \n",
    "    for m = 1:length(methods)\n",
    "        method = methods{m};\n",
    "        \n",
    "        % Time the optimization\n",
    "        tic;\n",
    "        \n",
    "        try\n",
    "            switch m\n",
    "                case 1  % Gradient Descent with adaptive step size\n",
    "                    [x_opt, f_opt, iters] = gradient_descent(prob.func, prob.grad, prob.init, 1e-8, 5000);\n",
    "                case 2  % Newton's Method\n",
    "                    % Use numerical Hessian for general case\n",
    "                    hess_func = @(x) numerical_hessian(prob.func, x);\n",
    "                    [x_opt, f_opt, iters] = newton_optimization(prob.func, prob.grad, hess_func, prob.init, 1e-8, 100);\n",
    "                case 3  % BFGS\n",
    "                    [x_opt, f_opt, iters] = bfgs_method(prob.func, prob.grad, prob.init, 1e-8, 1000);\n",
    "            end\n",
    "            \n",
    "            elapsed_time = toc;\n",
    "            error_to_opt = norm(x_opt - prob.optimum);\n",
    "            \n",
    "            % Determine status\n",
    "            if error_to_opt < 1e-4 && abs(f_opt - prob.opt_value) < 1e-6\n",
    "                status = 'SUCCESS';\n",
    "            elseif iters >= (m == 1 ? 5000 : (m == 2 ? 100 : 1000))\n",
    "                status = 'MAX_ITER';\n",
    "            else\n",
    "                status = 'PARTIAL';\n",
    "            end\n",
    "            \n",
    "        catch ME\n",
    "            elapsed_time = toc;\n",
    "            f_opt = NaN;\n",
    "            error_to_opt = NaN;\n",
    "            iters = NaN;\n",
    "            status = 'FAILED';\n",
    "        end\n",
    "        \n",
    "        % Store results\n",
    "        results(f, m).function = prob.name;\n",
    "        results(f, m).method = method;\n",
    "        results(f, m).iterations = iters;\n",
    "        results(f, m).final_value = f_opt;\n",
    "        results(f, m).error = error_to_opt;\n",
    "        results(f, m).time = elapsed_time;\n",
    "        results(f, m).status = status;\n",
    "        \n",
    "        % Print results\n",
    "        fprintf('%-12s | %-16s | %8d   | %14.8f   | %14.6e   | %9.4f   | %s\\n', ...\n",
    "                prob.name, method, iters, f_opt, error_to_opt, elapsed_time, status);\n",
    "    end\n",
    "    fprintf('---------------------------------------------------------------------------------------------------------------------\\n');\n",
    "end\n",
    "\n",
    "% Numerical Hessian function\n",
    "function H = numerical_hessian(func, x)\n",
    "    n = length(x);\n",
    "    H = zeros(n, n);\n",
    "    h = sqrt(eps);\n",
    "    \n",
    "    f0 = func(x);\n",
    "    \n",
    "    for i = 1:n\n",
    "        for j = 1:n\n",
    "            x_pp = x; x_pp(i) = x_pp(i) + h; x_pp(j) = x_pp(j) + h;\n",
    "            x_pm = x; x_pm(i) = x_pm(i) + h; x_pm(j) = x_pm(j) - h;\n",
    "            x_mp = x; x_mp(i) = x_mp(i) - h; x_mp(j) = x_mp(j) + h;\n",
    "            x_mm = x; x_mm(i) = x_mm(i) - h; x_mm(j) = x_mm(j) - h;\n",
    "            \n",
    "            H(i,j) = (func(x_pp) - func(x_pm) - func(x_mp) + func(x_mm)) / (4*h^2);\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "% Performance summary\n",
    "fprintf('\\n3. Method Performance Summary:\\n');\n",
    "\n",
    "success_rates = zeros(1, length(methods));\n",
    "avg_iterations = zeros(1, length(methods));\n",
    "avg_times = zeros(1, length(methods));\n",
    "\n",
    "for m = 1:length(methods)\n",
    "    method_results = results(:, m);\n",
    "    \n",
    "    % Success rate\n",
    "    successes = sum(strcmp({method_results.status}, 'SUCCESS'));\n",
    "    success_rates(m) = successes / length(test_functions) * 100;\n",
    "    \n",
    "    % Average iterations (only successful runs)\n",
    "    successful_iters = [method_results(strcmp({method_results.status}, 'SUCCESS')).iterations];\n",
    "    if ~isempty(successful_iters)\n",
    "        avg_iterations(m) = mean(successful_iters);\n",
    "    else\n",
    "        avg_iterations(m) = NaN;\n",
    "    end\n",
    "    \n",
    "    % Average time\n",
    "    avg_times(m) = mean([method_results.time]);\n",
    "end\n",
    "\n",
    "fprintf('   Method Performance Rankings:\\n');\n",
    "fprintf('   ========================================\\n');\n",
    "fprintf('   Method           | Success Rate | Avg Iterations | Avg Time\\n');\n",
    "fprintf('   ========================================\\n');\n",
    "\n",
    "for m = 1:length(methods)\n",
    "    fprintf('   %-16s | %10.1f%% | %12.1f   | %8.4fs\\n', ...\n",
    "            methods{m}, success_rates(m), avg_iterations(m), avg_times(m));\n",
    "end\n",
    "\n",
    "fprintf('\\n4. Recommendations:\\n');\n",
    "fprintf('   Based on benchmarking results:\\n');\n",
    "\n",
    "[~, best_success] = max(success_rates);\n",
    "[~, fastest_method] = min(avg_iterations(~isnan(avg_iterations)));\n",
    "[~, most_efficient] = min(avg_times);\n",
    "\n",
    "fprintf('   • Most Reliable: %s (%.1f%% success rate)\\n', methods{best_success}, success_rates(best_success));\n",
    "fprintf('   • Fastest Convergence: %s (%.1f average iterations)\\n', methods{fastest_method}, avg_iterations(fastest_method));\n",
    "fprintf('   • Most Time Efficient: %s (%.4fs average time)\\n', methods{most_efficient}, avg_times(most_efficient));\n",
    "\n",
    "fprintf('\\n   Selection Guidelines:\\n');\n",
    "fprintf('   • Use BFGS for general-purpose optimization (best balance)\\n');\n",
    "fprintf('   • Use Newton''s method when Hessian is available and cheap to compute\\n');\n",
    "fprintf('   • Use Gradient Descent for very large-scale problems or when memory is limited\\n');\n",
    "fprintf('   • Always try multiple starting points for non-convex problems\\n');\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**Optimization & Root-Finding Mastery Completed:**\n",
    "\n",
    "This comprehensive notebook covered all essential numerical optimization and root-finding techniques:\n",
    "\n",
    "- ✅ **Root-Finding**: Bisection, Newton-Raphson, secant, fixed-point iteration methods\n",
    "- ✅ **Nonlinear Systems**: Newton's method, Broyden's quasi-Newton for equation systems  \n",
    "- ✅ **Unconstrained Optimization**: Gradient descent, Newton's method, BFGS quasi-Newton\n",
    "- ✅ **Constrained Optimization**: Penalty methods, Lagrange multipliers, projected gradient\n",
    "- ✅ **Global Optimization**: Simulated annealing, genetic algorithms, multi-start, differential evolution\n",
    "\n",
    "**Key Algorithmic Insights:**\n",
    "1. **Convergence Rates**: Newton (quadratic) > Secant (superlinear) > Bisection (linear)\n",
    "2. **Robustness vs Speed**: Bisection most robust, Newton fastest but needs good initial guess\n",
    "3. **Quasi-Newton**: BFGS provides superlinear convergence without computing Hessian\n",
    "4. **Global vs Local**: Local methods fast but can miss global optimum\n",
    "5. **Constraint Handling**: Lagrange multipliers provide exact solutions when applicable\n",
    "\n",
    "**Professional Optimization Skills:**\n",
    "- Select appropriate algorithms based on problem characteristics\n",
    "- Implement robust convergence criteria and error handling\n",
    "- Understand trade-offs between accuracy, speed, and reliability\n",
    "- Apply global optimization for multi-modal problems\n",
    "- Handle constrained problems with KKT conditions\n",
    "\n",
    "**Real-World Applications:**\n",
    "- **Engineering**: Design optimization, control system tuning, parameter estimation\n",
    "- **Finance**: Portfolio optimization, risk management, option pricing models\n",
    "- **Machine Learning**: Training algorithms, hyperparameter optimization\n",
    "- **Scientific Computing**: Model fitting, inverse problems, calibration\n",
    "- **Operations Research**: Resource allocation, scheduling, logistics\n",
    "\n",
    "**Best Practices Established:**\n",
    "- Always verify convergence criteria and check solution quality\n",
    "- Use multiple starting points for complex optimization landscapes  \n",
    "- Implement proper line search and step size control\n",
    "- Validate solutions by checking optimality conditions\n",
    "- Consider problem scaling and conditioning for numerical stability\n",
    "\n",
    "**Next Steps:**\n",
    "- Apply these methods to domain-specific optimization problems\n",
    "- Explore modern metaheuristic algorithms for complex problems\n",
    "- Study convex optimization theory for guaranteed global solutions\n",
    "- Implement specialized methods for specific constraint types\n",
    "\n",
    "Your optimization toolkit is now research and industry ready!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
